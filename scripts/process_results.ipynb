{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset\n",
    "First process the synthetic dataset results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scattering_type', 'sub_dataset', 'model', 'score', 'stdev', 'ncomp',\n",
      "       'pca_var'],\n",
      "      dtype='object')\n",
      "Index(['sub_dataset', 'model', 'acc', 'stdev', 'hidden_dim', 'learning_rate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "RESULTS_DIR = 'results_sym'\n",
    "\n",
    "synth_scat = pd.read_csv(os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0.csv'))\n",
    "print(synth_scat.columns)\n",
    "\n",
    "synth_gnn = pd.read_csv(os.path.join(RESULTS_DIR, 'synthetic_GNN.csv'))\n",
    "print(synth_gnn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 wavelet\n",
      "gaussian\n",
      "  scattering_type model           score\n",
      "0            blis   MLP  $99.5 \\pm 0.3$\n",
      "1         modulus   MLP  $81.3 \\pm 7.3$\n",
      "camel\n",
      "  scattering_type model           score\n",
      "0            blis   MLP  $98.6 \\pm 0.4$\n",
      "1         modulus   MLP  $96.4 \\pm 1.3$\n"
     ]
    }
   ],
   "source": [
    "synth_scat_gaussian = synth_scat[synth_scat['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_scat_camel = synth_scat[synth_scat['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "print('W2 wavelet')\n",
    "for name, df in zip(['gaussian', 'camel'],[synth_scat_gaussian, synth_scat_camel]):\n",
    "    grouped = df.groupby(['scattering_type', 'model'])['score']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 Wavelet\n",
      "gaussian\n",
      "  scattering_type model            score\n",
      "0            blis   MLP  $100.0 \\pm 0.0$\n",
      "1         modulus   MLP   $92.8 \\pm 3.4$\n",
      "camel\n",
      "  scattering_type model           score\n",
      "0            blis   MLP  $97.7 \\pm 0.5$\n",
      "1         modulus   MLP  $97.1 \\pm 1.3$\n"
     ]
    }
   ],
   "source": [
    "synth_scat_W1 = pd.read_csv(os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0_W1.csv'))\n",
    "synth_scat_gaussian = synth_scat_W1[synth_scat_W1['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_scat_camel = synth_scat_W1[synth_scat_W1['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "print('W1 Wavelet')\n",
    "for name, df in zip(['gaussian', 'camel'],[synth_scat_gaussian, synth_scat_camel]):\n",
    "    grouped = df.groupby(['scattering_type', 'model'])['score']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian\n",
      "  model             acc\n",
      "0   GAT  $99.2 \\pm 0.5$\n",
      "1   GCN  $99.0 \\pm 0.4$\n",
      "2   GIN  $99.5 \\pm 0.2$\n",
      "camel\n",
      "  model             acc\n",
      "0   GAT  $91.6 \\pm 2.0$\n",
      "1   GCN  $91.7 \\pm 2.0$\n",
      "2   GIN  $91.3 \\pm 1.4$\n"
     ]
    }
   ],
   "source": [
    "synth_gnn_gaussian = synth_gnn[synth_gnn['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_gnn_camel = synth_gnn[synth_gnn['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "\n",
    "for name, df in zip(['gaussian', 'camel'], [synth_gnn_gaussian, synth_gnn_camel]):\n",
    "    grouped = df.groupby(['model'])['acc']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian\n",
      "  model             acc\n",
      "0   GPS  $95.4 \\pm 5.9$\n",
      "camel\n",
      "  model             acc\n",
      "0   GPS  $97.7 \\pm 0.9$\n"
     ]
    }
   ],
   "source": [
    "GPS_pd_synthetic = pd.read_csv(os.path.join(RESULTS_DIR, 'GPS_synthetic_GNN.csv'))\n",
    "synth_gnn_gaussian = GPS_pd_synthetic[GPS_pd_synthetic['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_gnn_camel = GPS_pd_synthetic[GPS_pd_synthetic['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "\n",
    "for name, df in zip(['gaussian', 'camel'], [synth_gnn_gaussian, synth_gnn_camel]):\n",
    "    grouped = df.groupby(['model'])['acc']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scattering and blis with layers 1,2,3\n",
      "gaussian\n",
      "  scattering_type wavelet_type            score\n",
      "0            blis           W1  $100.0 \\pm 0.0$\n",
      "1            blis           W2   $99.3 \\pm 0.2$\n",
      "2         modulus           W1   $99.9 \\pm 0.2$\n",
      "3         modulus           W2   $89.7 \\pm 3.6$\n",
      "camel\n",
      "  scattering_type wavelet_type           score\n",
      "0            blis           W1  $97.6 \\pm 0.7$\n",
      "1            blis           W2  $98.4 \\pm 0.8$\n",
      "2         modulus           W1  $96.7 \\pm 1.1$\n",
      "3         modulus           W2  $96.9 \\pm 1.0$\n"
     ]
    }
   ],
   "source": [
    "# process synthetic with skips\n",
    "print(\"scattering and blis with layers 1,2,3\")\n",
    "synthetic_W1_skips = pd.read_csv( os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0_W1_skips.csv' ) )\n",
    "synthetic_W2_skips = pd.read_csv( os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0_W2_skips.csv' ) )\n",
    "\n",
    "synth_skips = pd.concat([synthetic_W1_skips, synthetic_W2_skips], axis = 0).reset_index(drop=True)\n",
    "\n",
    "synth_skips_gaussian = synth_skips[synth_skips['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_skips_camel = synth_skips[synth_skips['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "for name, df in zip(['gaussian', 'camel'],[synth_skips_gaussian, synth_skips_camel]):\n",
    "    grouped = df.groupby(['scattering_type', 'wavelet_type'])['score']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scattering and blis with layers 1,2\n",
      "gaussian\n",
      "  scattering_type wavelet_type            score\n",
      "0            blis           W1  $100.0 \\pm 0.0$\n",
      "1            blis           W2   $99.7 \\pm 0.2$\n",
      "2         modulus           W1   $97.7 \\pm 1.0$\n",
      "3         modulus           W2   $88.3 \\pm 4.3$\n",
      "camel\n",
      "  scattering_type wavelet_type           score\n",
      "0            blis           W1  $96.7 \\pm 0.8$\n",
      "1            blis           W2  $97.4 \\pm 0.9$\n",
      "2         modulus           W1  $96.5 \\pm 1.2$\n",
      "3         modulus           W2  $96.8 \\pm 1.0$\n"
     ]
    }
   ],
   "source": [
    "# process synthetic with skips\n",
    "print(\"scattering and blis with layers 1,2\")\n",
    "synthetic_W1_skips = pd.read_csv( os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0_W1_skips2.csv' ) )\n",
    "synthetic_W2_skips = pd.read_csv( os.path.join(RESULTS_DIR, 'synthetic_results_pca_1.0_W2_skips2.csv' ) )\n",
    "\n",
    "synth_skips = pd.concat([synthetic_W1_skips, synthetic_W2_skips], axis = 0).reset_index(drop=True)\n",
    "\n",
    "synth_skips_gaussian = synth_skips[synth_skips['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "synth_skips_camel = synth_skips[synth_skips['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "for name, df in zip(['gaussian', 'camel'],[synth_skips_gaussian, synth_skips_camel]):\n",
    "    grouped = df.groupby(['scattering_type', 'wavelet_type'])['score']\n",
    "    mean_scores = grouped.mean().reset_index()\n",
    "    std_deviation = grouped.std().reset_index()\n",
    "\n",
    "    # Convert to percentages (without the percent sign) and format\n",
    "    formatted_results = mean_scores.copy()\n",
    "    formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                                \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "    print(name)\n",
    "    print(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partly cloudy dataset\n",
    "Next process the partly cloudy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['scattering_type', 'sub_dataset', 'model', 'score', 'stdev', 'ncomp',\n",
      "       'pca_var'],\n",
      "      dtype='object')\n",
      "Index(['sub_dataset', 'model', 'acc', 'stdev', 'hidden_dim', 'learning_rate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pc_scat = pd.read_csv(os.path.join(RESULTS_DIR, 'partly_cloudy_results_scattering_1.0.csv'))\n",
    "print(pc_scat.columns)\n",
    "\n",
    "pc_gnn = pd.read_csv(os.path.join(RESULTS_DIR, 'partly_cloudy_GNN.csv'))\n",
    "partly_cloudy_gnn = pd.read_csv(os.path.join(RESULTS_DIR,'partly_cloudy_GPS.csv'))\n",
    "pc_gnn = pd.concat([pc_gnn, partly_cloudy_gnn], ignore_index=True)\n",
    "print(pc_gnn.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy scattering W2\n",
      "  scattering_type model           score\n",
      "0            blis   MLP  $41.5 \\pm 5.5$\n",
      "1         modulus   MLP  $39.0 \\pm 5.8$\n"
     ]
    }
   ],
   "source": [
    "print('partly cloudy scattering W2')\n",
    "grouped = pc_scat.groupby(['scattering_type', 'model'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy GNNs\n",
      "  model             acc\n",
      "0   GAT  $37.3 \\pm 4.8$\n",
      "1   GCN  $37.5 \\pm 4.9$\n",
      "2   GIN  $37.1 \\pm 4.5$\n",
      "3   GPS  $42.0 \\pm 4.3$\n"
     ]
    }
   ],
   "source": [
    "grouped = pc_gnn.groupby(['model'])['acc']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "print('partly cloudy GNNs')\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sub_dataset', 'model', 'acc', 'stdev', 'hidden_dim', 'learning_rate'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "partly_cloudy_gnn.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll still process the partly cloudy data, but with the new sets of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy GNNs\n",
      "     label model             acc\n",
      "0      GUS   GAT  $62.0 \\pm 3.0$\n",
      "1      GUS   GCN  $61.9 \\pm 2.9$\n",
      "2      GUS   GIN  $59.4 \\pm 3.7$\n",
      "3      GUS   GPS  $57.7 \\pm 4.4$\n",
      "4   MENTAL   GAT  $85.5 \\pm 0.0$\n",
      "5   MENTAL   GCN  $85.5 \\pm 0.0$\n",
      "6   MENTAL   GIN  $85.4 \\pm 0.4$\n",
      "7   MENTAL   GPS  $80.1 \\pm 3.0$\n",
      "8     PAIN   GAT  $84.3 \\pm 0.0$\n",
      "9     PAIN   GCN  $84.3 \\pm 0.0$\n",
      "10    PAIN   GIN  $84.2 \\pm 0.3$\n",
      "11    PAIN   GPS  $78.4 \\pm 3.3$\n",
      "12    PECK   GAT  $57.8 \\pm 3.8$\n",
      "13    PECK   GCN  $57.6 \\pm 3.8$\n",
      "14    PECK   GIN  $56.8 \\pm 4.5$\n",
      "15    PECK   GPS  $58.5 \\pm 4.7$\n"
     ]
    }
   ],
   "source": [
    "labels = [\"GUS\", \"MENTAL\", \"PAIN\", \"PECK\"]\n",
    "wavelet_types = ['W1', \"W2\"]\n",
    "scattering_base_name = \"partly_cloudy_results_scattering_1.0_\"\n",
    "gnn_base_name = \"partly_cloudy_\"\n",
    "\n",
    "gnn_dfs = []\n",
    "# load the gnn dfs first\n",
    "for label in labels:\n",
    "    gnn_df = pd.read_csv(os.path.join(RESULTS_DIR, gnn_base_name + label + '.csv'))\n",
    "    # create a new column in the dataframe called label and set all of its values to \n",
    "    gnn_df['label'] = label\n",
    "    gnn_dfs.append(gnn_df)\n",
    "gnn_df_full = pd.concat(gnn_dfs, axis = 0).reset_index(drop=True)\n",
    "gnn_df_full\n",
    "#pc_scat_new_labels = pd.read_csv(os.path.join(RESULTS_DIR, 'partly_cloudy_results_scattering_1.0.csv'))\n",
    "\n",
    "grouped = gnn_df_full.groupby(['label','model'])['acc']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "print('partly cloudy GNNs')\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['scattering_type', 'sub_dataset', 'model', 'score', 'stdev', 'ncomp',\n",
       "       'task', 'pca_var'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(RESULTS_DIR, scattering_base_name+ 'W1_GUS.csv'))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label scattering_type wavelet model           score\n",
      "0      GUS            blis      W1   MLP  $56.6 \\pm 5.4$\n",
      "1      GUS            blis      W2   MLP  $57.2 \\pm 4.8$\n",
      "2      GUS         modulus      W1   MLP  $54.6 \\pm 4.7$\n",
      "3      GUS         modulus      W2   MLP  $55.5 \\pm 5.1$\n",
      "4   MENTAL            blis      W1   MLP  $79.3 \\pm 3.0$\n",
      "5   MENTAL            blis      W2   MLP  $79.7 \\pm 3.0$\n",
      "6   MENTAL         modulus      W1   MLP  $79.7 \\pm 2.8$\n",
      "7   MENTAL         modulus      W2   MLP  $80.1 \\pm 2.6$\n",
      "8     PAIN            blis      W1   MLP  $77.4 \\pm 3.2$\n",
      "9     PAIN            blis      W2   MLP  $77.9 \\pm 3.1$\n",
      "10    PAIN         modulus      W1   MLP  $78.3 \\pm 2.6$\n",
      "11    PAIN         modulus      W2   MLP  $78.7 \\pm 2.9$\n",
      "12    PECK            blis      W1   MLP  $56.9 \\pm 5.2$\n",
      "13    PECK            blis      W2   MLP  $56.8 \\pm 5.5$\n",
      "14    PECK         modulus      W1   MLP  $54.2 \\pm 5.1$\n",
      "15    PECK         modulus      W2   MLP  $54.6 \\pm 5.9$\n"
     ]
    }
   ],
   "source": [
    "# now do the same but for the wavelet classifiers \n",
    "pc_scat_dfs = [] \n",
    "for label in labels:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, scattering_base_name + wavelet_type + '_'+label+'.csv'))\n",
    "        df['wavelet'] = wavelet_type \n",
    "        df['label'] = label \n",
    "        pc_scat_dfs.append(df)\n",
    "\n",
    "pc_scat_dfs_full = pd.concat(pc_scat_dfs, axis = 0).reset_index(drop=True) \n",
    "grouped = pc_scat_dfs_full.groupby([\"label\",'scattering_type', \"wavelet\",'model'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the partly cloudy data with smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy EMOTION 3 with gaussian smoothing\n",
      "partly cloudy GNNs\n",
      "  model             acc\n",
      "0   GAT  $39.3 \\pm 6.0$\n",
      "1   GCN  $39.3 \\pm 5.9$\n",
      "2   GIN  $42.1 \\pm 6.0$\n",
      "3   GPS  $56.4 \\pm 4.3$\n"
     ]
    }
   ],
   "source": [
    "# read in GNN data \n",
    "print(\"partly cloudy EMOTION 3 with gaussian smoothing\")\n",
    "models = ['GAT', 'GCN', 'GIN', 'GPS'] # ,'GPS' \n",
    "cloudy_gnns = []\n",
    "for model in models:\n",
    "    file_name = f'partly_cloudy_EMOTION3_{model}_smoothed.csv'\n",
    "    df = pd.read_csv(os.path.join(RESULTS_DIR, file_name))\n",
    "    cloudy_gnns.append(df) \n",
    "\n",
    "pc_gnn_smooth = pd.concat(cloudy_gnns, axis = 0).reset_index(drop = True)\n",
    "grouped = pc_gnn_smooth.groupby(['model'])['acc']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "print('partly cloudy GNNs')\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy EMOTION 3 scattering classification w gaussian smoothing\n",
      "  scattering_type wavelet model           score\n",
      "0            blis      W1   MLP  $67.1 \\pm 4.3$\n",
      "1            blis      W2   MLP  $68.3 \\pm 3.6$\n",
      "2         modulus      W1   MLP  $60.6 \\pm 4.9$\n",
      "3         modulus      W2   MLP  $62.3 \\pm 5.1$\n"
     ]
    }
   ],
   "source": [
    "# read in scattering data \n",
    "print(\"partly cloudy EMOTION 3 scattering classification w gaussian smoothing\")\n",
    "scattering_types = ['blis', 'modulus'] \n",
    "wavelets = ['W1', 'W2']\n",
    "\n",
    "pc_smooth_scattering = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet in wavelets: \n",
    "        if scattering_type == 'blis':\n",
    "            file_name = f'partly_cloudy_smoothed_{scattering_type}_{wavelet}_EMOTION3.csv'\n",
    "        else:\n",
    "            file_name = f'partly_cloudy_smoothed_{scattering_type}_{wavelet}_EMOTION3_skip2.csv'\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, file_name))\n",
    "        df['wavelet'] = wavelet\n",
    "        pc_smooth_scattering.append(df)\n",
    "\n",
    "\n",
    "pc_scat_dfs_full = pd.concat(pc_smooth_scattering, axis = 0).reset_index(drop=True) \n",
    "grouped = pc_scat_dfs_full.groupby(['scattering_type', \"wavelet\",'model'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the partly cloudy scattering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy scattering W1\n",
      "  scattering_type model           score\n",
      "0            blis   MLP  $41.1 \\pm 5.0$\n",
      "1         modulus   MLP  $37.3 \\pm 5.7$\n"
     ]
    }
   ],
   "source": [
    "pc_scat_w1 = pd.read_csv(os.path.join(RESULTS_DIR, 'partly_cloudy_results_scattering_1.0_W1.csv'))\n",
    "\n",
    "print('partly cloudy scattering W1')\n",
    "grouped = pc_scat_w1.groupby(['scattering_type', 'model'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy results on EMOTION3, layers 0,1,2,3\n",
      "  scattering_type wavelet_type           score\n",
      "0            blis           W1  $41.0 \\pm 5.4$\n",
      "1            blis           W2  $41.1 \\pm 5.7$\n",
      "2         modulus           W1  $38.9 \\pm 5.0$\n",
      "3         modulus           W2  $39.5 \\pm 5.6$\n"
     ]
    }
   ],
   "source": [
    "# process the partly cloudy results with skip connections \n",
    "print(\"partly cloudy results on EMOTION3, layers 0,1,2,3\")\n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelet_types = ['W1', 'W2']\n",
    "dfs = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        file_name = f'partly_cloudy_{scattering_type}_{wavelet_type}_EMOTION3_skip3.csv'\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, file_name))\n",
    "        df['wavelet_type'] = wavelet_type \n",
    "        dfs.append(df)\n",
    "\n",
    "pc_skip3 = pd.concat(dfs, axis = 0).reset_index(drop = True)\n",
    "grouped = pc_skip3.groupby(['scattering_type', 'wavelet_type'])['score'] \n",
    "mean_scores = grouped.mean().reset_index() \n",
    "std_deviation = grouped.std().reset_index() \n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy results on EMOTION3, layers 0,1,2\n",
      "  scattering_type wavelet_type           score\n",
      "0            blis           W1  $40.1 \\pm 5.5$\n",
      "1            blis           W2  $40.3 \\pm 5.6$\n",
      "2         modulus           W1  $40.3 \\pm 5.3$\n",
      "3         modulus           W2  $40.7 \\pm 5.8$\n"
     ]
    }
   ],
   "source": [
    "# process the partly cloudy results with skip connections \n",
    "print(\"partly cloudy results on EMOTION3, layers 0,1,2\")\n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelet_types = ['W1', 'W2']\n",
    "dfs = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        file_name = f'partly_cloudy_{scattering_type}_{wavelet_type}_EMOTION3_skip.csv'\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, file_name))\n",
    "        df['wavelet_type'] = wavelet_type \n",
    "        dfs.append(df)\n",
    "\n",
    "pc_skip3 = pd.concat(dfs, axis = 0).reset_index(drop = True)\n",
    "grouped = pc_skip3.groupby(['scattering_type', 'wavelet_type'])['score'] \n",
    "mean_scores = grouped.mean().reset_index() \n",
    "std_deviation = grouped.std().reset_index() \n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic dataset\n",
    "Now process the traffic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sub_dataset', 'model', 'acc', 'stdev', 'hidden_dim', 'learning_rate',\n",
      "       'task'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "traffic_gnn_03 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GNN_PEMS03.csv'))\n",
    "traffic_gnn_03_GPS = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GPS_PEMS03.csv'))\n",
    "traffic_gnn_03 = pd.concat([traffic_gnn_03, traffic_gnn_03_GPS], ignore_index=True)\n",
    "\n",
    "traffic_gnn_04 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GNN_PEMS04.csv'))\n",
    "traffic_gnn_04_GPS = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GPS_PEMS04.csv'))\n",
    "traffic_gnn_04 = pd.concat([traffic_gnn_04, traffic_gnn_04_GPS], ignore_index=True)\n",
    "\n",
    "traffic_gnn_07 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GNN_PEMS07.csv'))\n",
    "traffic_gnn_07_GPS = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GPS_PEMS07.csv'))\n",
    "traffic_gnn_07 = pd.concat([traffic_gnn_07, traffic_gnn_07_GPS], ignore_index=True)\n",
    "\n",
    "traffic_gnn_08 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GNN_PEMS08.csv'))\n",
    "traffic_gnn_08_GPS = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_GPS_PEMS08.csv'))\n",
    "traffic_gnn_08 = pd.concat([traffic_gnn_08, traffic_gnn_08_GPS], ignore_index=True)\n",
    "\n",
    "traffic_gnn = pd.concat([traffic_gnn_03, traffic_gnn_04, traffic_gnn_07, traffic_gnn_08], ignore_index=True)\n",
    "\n",
    "print(traffic_gnn.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic GNNs\n",
      "   sub_dataset  task model              acc\n",
      "0       PEMS03   DAY   GPS   $49.6 \\pm 0.3$\n",
      "1       PEMS03  HOUR   GPS   $57.4 \\pm 0.1$\n",
      "2       PEMS03  WEEK   GPS   $31.9 \\pm 0.4$\n",
      "3       PEMS04   DAY   GPS   $67.0 \\pm 5.3$\n",
      "4       PEMS04  HOUR   GPS   $66.5 \\pm 0.3$\n",
      "5       PEMS04  WEEK   GPS   $31.7 \\pm 0.2$\n",
      "6       PEMS07   DAY   GPS   $27.7 \\pm 1.9$\n",
      "7       PEMS07  HOUR   GPS   $39.9 \\pm 2.7$\n",
      "8       PEMS07  WEEK   GPS   $30.4 \\pm 0.6$\n",
      "9       PEMS08   DAY   GPS   $67.9 \\pm 0.6$\n",
      "10      PEMS08  HOUR   GPS   $67.7 \\pm 0.8$\n",
      "11      PEMS08  WEEK   GPS  $62.3 \\pm 16.7$\n"
     ]
    }
   ],
   "source": [
    "grouped = traffic_gnn[traffic_gnn['model']=='GPS'].groupby(['sub_dataset', 'task','model'\n",
    "                               ])['acc']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "grouped_stdev = traffic_gnn.groupby(['task','sub_dataset', 'model'\n",
    "                               ])['stdev']\n",
    "std_deviation = grouped_stdev.mean().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['stdev'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "print('traffic GNNs')\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the traffic scattering results\n",
    "traffic_day_w2 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_DAY_pca_1.0_layers_3_moments_1_W2.csv'))\n",
    "traffic_hour_w2 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_HOUR_pca_1.0_layers_3_moments_1_W2.csv'))\n",
    "traffic_week_w2 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_WEEK_pca_1.0_layers_3_moments_1_W2.csv'))\n",
    "\n",
    "traffic_day_w1 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_DAY_pca_1.0_layers_3_moments_1_W1.csv')) \n",
    "traffic_hour_w1 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_HOUR_pca_1.0_layers_3_moments_1_W1.csv'))\n",
    "traffic_week_w1 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_WEEK_pca_1.0_layers_3_moments_1_W1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process the new data frames\n",
    "dataframes_and_tasks = [(traffic_day_w2, 'DAY', 'W2'), \n",
    "                        (traffic_hour_w2, 'HOUR', 'W2'), \n",
    "                        (traffic_week_w2, 'WEEK', 'W2'),\n",
    "                        (traffic_day_w1, 'DAY', 'W1'),\n",
    "                        (traffic_hour_w1, 'HOUR', 'W1'), \n",
    "                        (traffic_week_w1, 'WEEK', 'W1')]\n",
    "\n",
    "for df, task,wavelet_type in dataframes_and_tasks:\n",
    "    # Add 'task' column\n",
    "    df['task'] = task\n",
    "    # Modify 'model' column\n",
    "    df['model'] = wavelet_type + df['scattering_type'] + df['model']\n",
    "    # Rename 'score' to 'acc'\n",
    "    df.rename(columns={'score': 'acc'}, inplace=True)\n",
    "    df['acc'] = df['acc'] * 100 # rescale to be in percent\n",
    "    df['stdev'] = df['stdev'] * 100 # rescale to be in percent\n",
    "\n",
    "# Step 2: Merge the processed data frames with traffic_gnn\n",
    "# First, retain only the columns you specified for each data frame\n",
    "cols_to_retain = ['sub_dataset', 'model', 'acc', 'stdev', 'task']\n",
    "traffic_gnn = traffic_gnn[cols_to_retain]\n",
    "traffic_day_w2 = traffic_day_w2[cols_to_retain]\n",
    "traffic_hour_w2 = traffic_hour_w2[cols_to_retain]\n",
    "traffic_week_w2 = traffic_week_w2[cols_to_retain]\n",
    "traffic_day_w1 = traffic_day_w1[cols_to_retain]\n",
    "traffic_hour_w1 = traffic_hour_w1[cols_to_retain]\n",
    "traffic_week_w1 = traffic_week_w1[cols_to_retain]\n",
    "\n",
    "# Now, concatenate all data frames to get traffic_full\n",
    "traffic_full = pd.concat([traffic_gnn, traffic_day_w2, traffic_hour_w2, traffic_week_w2, traffic_day_w1, traffic_hour_w1, traffic_week_w1], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic GNNs\n",
      "   sub_dataset  task         model              acc\n",
      "0       PEMS03   DAY           GAT   $14.1 \\pm 0.3$\n",
      "1       PEMS03   DAY           GCN   $14.1 \\pm 0.1$\n",
      "2       PEMS03   DAY           GIN   $14.3 \\pm 0.4$\n",
      "3       PEMS03   DAY           GPS   $49.6 \\pm 5.3$\n",
      "4       PEMS03   DAY     W1blisMLP   $53.1 \\pm 1.3$\n",
      "..         ...   ...           ...              ...\n",
      "91      PEMS08  WEEK           GPS  $62.3 \\pm 26.0$\n",
      "92      PEMS08  WEEK     W1blisMLP   $94.6 \\pm 1.3$\n",
      "93      PEMS08  WEEK  W1modulusMLP   $94.3 \\pm 0.5$\n",
      "94      PEMS08  WEEK     W2blisMLP   $95.4 \\pm 1.4$\n",
      "95      PEMS08  WEEK  W2modulusMLP   $95.4 \\pm 1.9$\n",
      "\n",
      "[96 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group by desired columns and compute both the mean and standard deviation\n",
    "grouped_mean = traffic_full.groupby(['sub_dataset', 'task', 'model'])['acc'].mean().reset_index()\n",
    "grouped_std = traffic_full.groupby(['sub_dataset', 'task', 'model'])['stdev'].mean().reset_index()\n",
    "\n",
    "# Format the results\n",
    "formatted_results = grouped_mean.copy()\n",
    "formatted_results['acc'] = \"$\" + (grouped_mean['acc']).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (grouped_std['stdev']).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print('traffic GNNs')\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sub_dataset  task         model             acc\n",
      "4       PEMS03   DAY     W1blisMLP  $53.1 \\pm 1.3$\n",
      "5       PEMS03   DAY  W1modulusMLP  $51.7 \\pm 1.4$\n",
      "12      PEMS03  HOUR     W1blisMLP  $63.1 \\pm 2.2$\n",
      "13      PEMS03  HOUR  W1modulusMLP  $63.7 \\pm 1.1$\n",
      "20      PEMS03  WEEK     W1blisMLP  $54.8 \\pm 1.8$\n",
      "21      PEMS03  WEEK  W1modulusMLP  $58.0 \\pm 1.3$\n",
      "28      PEMS04   DAY     W1blisMLP  $88.4 \\pm 1.6$\n",
      "29      PEMS04   DAY  W1modulusMLP  $87.6 \\pm 0.7$\n",
      "36      PEMS04  HOUR     W1blisMLP  $83.3 \\pm 0.8$\n",
      "37      PEMS04  HOUR  W1modulusMLP  $83.0 \\pm 1.1$\n",
      "44      PEMS04  WEEK     W1blisMLP  $91.3 \\pm 1.4$\n",
      "45      PEMS04  WEEK  W1modulusMLP  $91.1 \\pm 0.9$\n",
      "52      PEMS07   DAY     W1blisMLP  $72.9 \\pm 1.5$\n",
      "53      PEMS07   DAY  W1modulusMLP  $62.0 \\pm 1.5$\n",
      "60      PEMS07  HOUR     W1blisMLP  $63.5 \\pm 1.1$\n",
      "61      PEMS07  HOUR  W1modulusMLP  $58.6 \\pm 0.6$\n",
      "68      PEMS07  WEEK     W1blisMLP  $76.8 \\pm 2.0$\n",
      "69      PEMS07  WEEK  W1modulusMLP  $66.3 \\pm 1.6$\n",
      "76      PEMS08   DAY     W1blisMLP  $94.3 \\pm 1.1$\n",
      "77      PEMS08   DAY  W1modulusMLP  $92.5 \\pm 0.8$\n",
      "84      PEMS08  HOUR     W1blisMLP  $84.0 \\pm 0.8$\n",
      "85      PEMS08  HOUR  W1modulusMLP  $83.5 \\pm 1.2$\n",
      "92      PEMS08  WEEK     W1blisMLP  $94.6 \\pm 1.3$\n",
      "93      PEMS08  WEEK  W1modulusMLP  $94.3 \\pm 0.5$\n"
     ]
    }
   ],
   "source": [
    "print(formatted_results[formatted_results['model'].str.startswith('W1')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sub_dataset label scattering_type wavelet layer_list           score\n",
      "0       PEMS03   DAY            blis      W1      0,1,2  $46.5 \\pm 1.0$\n",
      "1       PEMS03   DAY            blis      W2      0,1,2  $47.9 \\pm 1.3$\n",
      "2       PEMS03   DAY         modulus      W1      0,1,2  $45.6 \\pm 0.7$\n",
      "3       PEMS03   DAY         modulus      W2      0,1,2  $49.5 \\pm 0.9$\n",
      "4       PEMS03  HOUR            blis      W1      0,1,2  $59.5 \\pm 0.8$\n",
      "5       PEMS03  HOUR            blis      W2      0,1,2  $60.8 \\pm 2.2$\n",
      "6       PEMS03  HOUR         modulus      W1      0,1,2  $58.2 \\pm 0.8$\n",
      "7       PEMS03  HOUR         modulus      W2      0,1,2  $60.4 \\pm 0.5$\n",
      "8       PEMS03  WEEK            blis      W1      0,1,2  $46.6 \\pm 0.8$\n",
      "9       PEMS03  WEEK            blis      W2      0,1,2  $51.5 \\pm 1.2$\n",
      "10      PEMS03  WEEK         modulus      W1      0,1,2  $46.4 \\pm 1.3$\n",
      "11      PEMS03  WEEK         modulus      W2      0,1,2  $51.4 \\pm 1.0$\n",
      "12      PEMS04   DAY            blis      W1      0,1,2  $85.1 \\pm 0.9$\n",
      "13      PEMS04   DAY            blis      W2      0,1,2  $87.2 \\pm 0.7$\n",
      "14      PEMS04   DAY         modulus      W1      0,1,2  $82.8 \\pm 1.0$\n",
      "15      PEMS04   DAY         modulus      W2      0,1,2  $85.3 \\pm 0.8$\n",
      "16      PEMS04  HOUR            blis      W1      0,1,2  $80.1 \\pm 0.4$\n",
      "17      PEMS04  HOUR            blis      W2      0,1,2  $81.9 \\pm 1.4$\n",
      "18      PEMS04  HOUR         modulus      W1      0,1,2  $78.9 \\pm 0.5$\n",
      "19      PEMS04  HOUR         modulus      W2      0,1,2  $80.9 \\pm 0.4$\n",
      "20      PEMS04  WEEK            blis      W1      0,1,2  $87.0 \\pm 0.8$\n",
      "21      PEMS04  WEEK            blis      W2      0,1,2  $87.7 \\pm 1.3$\n",
      "22      PEMS04  WEEK         modulus      W1      0,1,2  $83.7 \\pm 0.8$\n",
      "23      PEMS04  WEEK         modulus      W2      0,1,2  $86.2 \\pm 1.2$\n",
      "24      PEMS07   DAY            blis      W1      0,1,2  $60.9 \\pm 0.8$\n",
      "25      PEMS07   DAY            blis      W2      0,1,2  $59.5 \\pm 0.9$\n",
      "26      PEMS07   DAY         modulus      W1      0,1,2  $53.3 \\pm 0.9$\n",
      "27      PEMS07   DAY         modulus      W2      0,1,2  $55.2 \\pm 1.2$\n",
      "28      PEMS07  HOUR            blis      W1      0,1,2  $59.1 \\pm 1.3$\n",
      "29      PEMS07  HOUR            blis      W2      0,1,2  $58.3 \\pm 1.6$\n",
      "30      PEMS07  HOUR         modulus      W1      0,1,2  $54.0 \\pm 0.6$\n",
      "31      PEMS07  HOUR         modulus      W2      0,1,2  $54.3 \\pm 0.7$\n",
      "32      PEMS07  WEEK            blis      W1      0,1,2  $64.1 \\pm 0.9$\n",
      "33      PEMS07  WEEK            blis      W2      0,1,2  $67.2 \\pm 1.2$\n",
      "34      PEMS07  WEEK         modulus      W1      0,1,2  $56.9 \\pm 1.3$\n",
      "35      PEMS07  WEEK         modulus      W2      0,1,2  $61.6 \\pm 1.0$\n",
      "36      PEMS08   DAY            blis      W1      0,1,2  $90.9 \\pm 1.0$\n",
      "37      PEMS08   DAY            blis      W2      0,1,2  $91.7 \\pm 1.0$\n",
      "38      PEMS08   DAY         modulus      W1      0,1,2  $88.9 \\pm 1.2$\n",
      "39      PEMS08   DAY         modulus      W2      0,1,2  $91.8 \\pm 0.4$\n",
      "40      PEMS08  HOUR            blis      W1      0,1,2  $81.4 \\pm 0.4$\n",
      "41      PEMS08  HOUR            blis      W2      0,1,2  $82.2 \\pm 1.1$\n",
      "42      PEMS08  HOUR         modulus      W1      0,1,2  $80.1 \\pm 1.0$\n",
      "43      PEMS08  HOUR         modulus      W2      0,1,2  $81.6 \\pm 0.7$\n",
      "44      PEMS08  WEEK            blis      W1      0,1,2  $90.5 \\pm 0.9$\n",
      "45      PEMS08  WEEK            blis      W2      0,1,2  $93.1 \\pm 2.0$\n",
      "46      PEMS08  WEEK         modulus      W1      0,1,2  $88.6 \\pm 0.5$\n",
      "47      PEMS08  WEEK         modulus      W2      0,1,2  $90.7 \\pm 0.9$\n"
     ]
    }
   ],
   "source": [
    "# process the traffic data with skip connections \n",
    "traffic_day_w2 = pd.read_csv(os.path.join(RESULTS_DIR, 'traffic_results_DAY_pca_1.0_layers_3_moments_1_W2.csv'))\n",
    "\n",
    "datasets = ['PEMS03', 'PEMS04', 'PEMS07', 'PEMS08']\n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelets = ['W1', 'W2']\n",
    "df_traffic = []\n",
    "\n",
    "# HOUR, DAY, WEEK\n",
    "\n",
    "for dataset in datasets:\n",
    "    for scattering_type in scattering_types:\n",
    "        for wavelet in wavelets:\n",
    "            filename = f'traffic_{dataset}_skip_{scattering_type}_{wavelet}.csv'\n",
    "            df = pd.read_csv(os.path.join(RESULTS_DIR, filename))\n",
    "            df['wavelet'] = wavelet \n",
    "            df['label'] = ['HOUR', 'DAY', 'WEEK']\n",
    "            df['score'] = df['score'] * 100 \n",
    "            df['stdev'] = df['stdev'] * 100\n",
    "            df_traffic.append(df)\n",
    "\n",
    "df_traffic = pd.concat(df_traffic, axis = 0).reset_index(drop=True)\n",
    "\n",
    "grouped_score = df_traffic.groupby(['sub_dataset', 'label', 'scattering_type','wavelet','layer_list'])['score']\n",
    "grouped_stdev = df_traffic.groupby(['sub_dataset', 'label', 'scattering_type', 'wavelet','layer_list'])['stdev']\n",
    "\n",
    "mean_scores = grouped_score.mean().reset_index()\n",
    "mean_stdevs = grouped_stdev.mean().reset_index()\n",
    "\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score']).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (mean_stdevs['stdev']).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sub_dataset label wavelet layer_list  difference\n",
      "0       PEMS03  HOUR      W1      0,1,2   -1.307223\n",
      "1       PEMS03   DAY      W1      0,1,2   -0.895219\n",
      "2       PEMS03  WEEK      W1      0,1,2   -0.157681\n",
      "3       PEMS03  HOUR      W2      0,1,2   -0.386572\n",
      "4       PEMS03   DAY      W2      0,1,2    1.668362\n",
      "5       PEMS03  WEEK      W2      0,1,2   -0.050865\n",
      "6       PEMS04  HOUR      W1      0,1,2   -1.169086\n",
      "7       PEMS04   DAY      W1      0,1,2   -2.220479\n",
      "8       PEMS04  WEEK      W1      0,1,2   -3.248333\n",
      "9       PEMS04  HOUR      W2      0,1,2   -0.972931\n",
      "10      PEMS04   DAY      W2      0,1,2   -1.890938\n",
      "11      PEMS04  WEEK      W2      0,1,2   -1.435857\n",
      "12      PEMS07  HOUR      W1      0,1,2   -5.059046\n",
      "13      PEMS07   DAY      W1      0,1,2   -7.633444\n",
      "14      PEMS07  WEEK      W1      0,1,2   -7.184695\n",
      "15      PEMS07  HOUR      W2      0,1,2   -4.034010\n",
      "16      PEMS07   DAY      W2      0,1,2   -4.369391\n",
      "17      PEMS07  WEEK      W2      0,1,2   -5.578649\n",
      "18      PEMS08  HOUR      W1      0,1,2   -1.298992\n",
      "19      PEMS08   DAY      W1      0,1,2   -2.000747\n",
      "20      PEMS08  WEEK      W1      0,1,2   -1.911161\n",
      "21      PEMS08  HOUR      W2      0,1,2   -0.597238\n",
      "22      PEMS08   DAY      W2      0,1,2    0.074655\n",
      "23      PEMS08  WEEK      W2      0,1,2   -2.441209\n"
     ]
    }
   ],
   "source": [
    "df_blis = df_traffic[df_traffic['scattering_type'] == 'blis'].copy() \n",
    "df_modulus = df_traffic[df_traffic['scattering_type'] == 'modulus'].copy()\n",
    "\n",
    "# Merge on shared columns and calculate difference\n",
    "merged = pd.merge(df_blis, df_modulus, on=['sub_dataset', 'label', 'wavelet', 'layer_list'], suffixes=('_blis', '_modulus'))\n",
    "merged['difference'] = merged['score_modulus'] - merged['score_blis']\n",
    "\n",
    "print(merged[['sub_dataset', 'label', 'wavelet', 'layer_list', 'difference']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for traffic, skip 3\n",
      "   sub_dataset label scattering_type wavelet layer_list           score\n",
      "0       PEMS03   DAY            blis      W1    0,1,2,3  $52.2 \\pm 2.0$\n",
      "1       PEMS03   DAY            blis      W2    0,1,2,3  $55.9 \\pm 2.6$\n",
      "2       PEMS03   DAY         modulus      W1    0,1,2,3  $55.7 \\pm 0.6$\n",
      "3       PEMS03   DAY         modulus      W2    0,1,2,3  $62.1 \\pm 0.9$\n",
      "4       PEMS03  HOUR            blis      W1    0,1,2,3  $64.3 \\pm 1.3$\n",
      "5       PEMS03  HOUR            blis      W2    0,1,2,3  $66.7 \\pm 2.7$\n",
      "6       PEMS03  HOUR         modulus      W1    0,1,2,3  $65.0 \\pm 2.2$\n",
      "7       PEMS03  HOUR         modulus      W2    0,1,2,3  $69.4 \\pm 1.2$\n",
      "8       PEMS03  WEEK            blis      W1    0,1,2,3  $54.6 \\pm 0.9$\n",
      "9       PEMS03  WEEK            blis      W2    0,1,2,3  $61.9 \\pm 3.1$\n",
      "10      PEMS03  WEEK         modulus      W1    0,1,2,3  $60.3 \\pm 0.5$\n",
      "11      PEMS03  WEEK         modulus      W2    0,1,2,3  $65.5 \\pm 2.5$\n",
      "12      PEMS04   DAY            blis      W1    0,1,2,3  $89.5 \\pm 2.6$\n",
      "13      PEMS04   DAY            blis      W2    0,1,2,3  $89.2 \\pm 2.3$\n",
      "14      PEMS04   DAY         modulus      W1    0,1,2,3  $90.8 \\pm 1.4$\n",
      "15      PEMS04   DAY         modulus      W2    0,1,2,3  $91.7 \\pm 0.6$\n",
      "16      PEMS04  HOUR            blis      W1    0,1,2,3  $83.9 \\pm 0.8$\n",
      "17      PEMS04  HOUR            blis      W2    0,1,2,3  $84.2 \\pm 1.1$\n",
      "18      PEMS04  HOUR         modulus      W1    0,1,2,3  $84.3 \\pm 0.4$\n",
      "19      PEMS04  HOUR         modulus      W2    0,1,2,3  $86.6 \\pm 0.9$\n",
      "20      PEMS04  WEEK            blis      W1    0,1,2,3  $90.4 \\pm 2.9$\n",
      "21      PEMS04  WEEK            blis      W2    0,1,2,3  $92.3 \\pm 1.4$\n",
      "22      PEMS04  WEEK         modulus      W1    0,1,2,3  $92.2 \\pm 1.4$\n",
      "23      PEMS04  WEEK         modulus      W2    0,1,2,3  $92.6 \\pm 1.5$\n",
      "24      PEMS07   DAY            blis      W1    0,1,2,3  $72.6 \\pm 1.0$\n",
      "25      PEMS07   DAY            blis      W2    0,1,2,3  $66.9 \\pm 1.0$\n",
      "26      PEMS07   DAY         modulus      W1    0,1,2,3  $66.5 \\pm 0.5$\n",
      "27      PEMS07   DAY         modulus      W2    0,1,2,3  $70.5 \\pm 1.8$\n",
      "28      PEMS07  HOUR            blis      W1    0,1,2,3  $64.7 \\pm 0.7$\n",
      "29      PEMS07  HOUR            blis      W2    0,1,2,3  $63.5 \\pm 2.1$\n",
      "30      PEMS07  HOUR         modulus      W1    0,1,2,3  $63.0 \\pm 1.0$\n",
      "31      PEMS07  HOUR         modulus      W2    0,1,2,3  $64.6 \\pm 1.3$\n",
      "32      PEMS07  WEEK            blis      W1    0,1,2,3  $75.9 \\pm 1.6$\n",
      "33      PEMS07  WEEK            blis      W2    0,1,2,3  $76.9 \\pm 1.8$\n",
      "34      PEMS07  WEEK         modulus      W1    0,1,2,3  $70.2 \\pm 0.9$\n",
      "35      PEMS07  WEEK         modulus      W2    0,1,2,3  $75.3 \\pm 1.6$\n",
      "36      PEMS08   DAY            blis      W1    0,1,2,3  $93.2 \\pm 1.7$\n",
      "37      PEMS08   DAY            blis      W2    0,1,2,3  $92.7 \\pm 2.9$\n",
      "38      PEMS08   DAY         modulus      W1    0,1,2,3  $94.6 \\pm 0.6$\n",
      "39      PEMS08   DAY         modulus      W2    0,1,2,3  $95.6 \\pm 0.6$\n",
      "40      PEMS08  HOUR            blis      W1    0,1,2,3  $84.7 \\pm 0.6$\n",
      "41      PEMS08  HOUR            blis      W2    0,1,2,3  $85.2 \\pm 2.1$\n",
      "42      PEMS08  HOUR         modulus      W1    0,1,2,3  $85.1 \\pm 1.2$\n",
      "43      PEMS08  HOUR         modulus      W2    0,1,2,3  $85.6 \\pm 1.3$\n",
      "44      PEMS08  WEEK            blis      W1    0,1,2,3  $93.9 \\pm 2.4$\n",
      "45      PEMS08  WEEK            blis      W2    0,1,2,3  $94.8 \\pm 1.7$\n",
      "46      PEMS08  WEEK         modulus      W1    0,1,2,3  $94.9 \\pm 0.8$\n",
      "47      PEMS08  WEEK         modulus      W2    0,1,2,3  $96.0 \\pm 0.6$\n"
     ]
    }
   ],
   "source": [
    "# now do the same thing as above but for the layers 0,1,2,3 on traffic \n",
    "print('results for traffic, skip 3')\n",
    "\n",
    "datasets = ['PEMS03', 'PEMS04', 'PEMS07', 'PEMS08']\n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelets = ['W1', 'W2']\n",
    "df_traffic = []\n",
    "\n",
    "# HOUR, DAY, WEEK\n",
    "\n",
    "for dataset in datasets:\n",
    "    for scattering_type in scattering_types:\n",
    "        for wavelet in wavelets:\n",
    "            filename = f'traffic_{dataset}_skip3_{scattering_type}_{wavelet}.csv'\n",
    "            df = pd.read_csv(os.path.join(RESULTS_DIR, filename))\n",
    "            df['wavelet'] = wavelet \n",
    "            df['label'] = ['HOUR', 'DAY', 'WEEK']\n",
    "            df['score'] = df['score'] * 100 \n",
    "            df['stdev'] = df['stdev'] * 100\n",
    "            df_traffic.append(df)\n",
    "\n",
    "df_traffic = pd.concat(df_traffic, axis = 0).reset_index(drop=True)\n",
    "\n",
    "grouped_score = df_traffic.groupby(['sub_dataset', 'label', 'scattering_type','wavelet','layer_list'])['score']\n",
    "grouped_stdev = df_traffic.groupby(['sub_dataset', 'label', 'scattering_type', 'wavelet','layer_list'])['stdev']\n",
    "\n",
    "mean_scores = grouped_score.mean().reset_index()\n",
    "mean_stdevs = grouped_stdev.mean().reset_index()\n",
    "\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score']).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (mean_stdevs['stdev']).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modulus - blis for skip 3\n",
      "   sub_dataset label wavelet layer_list  difference\n",
      "0       PEMS03  HOUR      W1    0,1,2,3    0.686673\n",
      "1       PEMS03   DAY      W1    0,1,2,3    3.540183\n",
      "2       PEMS03  WEEK      W1    0,1,2,3    5.752798\n",
      "3       PEMS03  HOUR      W2    0,1,2,3    2.690743\n",
      "4       PEMS03   DAY      W2    0,1,2,3    6.215666\n",
      "5       PEMS03  WEEK      W2    0,1,2,3    3.631740\n",
      "6       PEMS04  HOUR      W1    0,1,2,3    0.447234\n",
      "7       PEMS04   DAY      W1    0,1,2,3    1.318164\n",
      "8       PEMS04  WEEK      W1    0,1,2,3    1.812475\n",
      "9       PEMS04  HOUR      W2    0,1,2,3    2.385249\n",
      "10      PEMS04   DAY      W2    0,1,2,3    2.487250\n",
      "11      PEMS04  WEEK      W2    0,1,2,3    0.298156\n",
      "12      PEMS07  HOUR      W1    0,1,2,3   -1.676901\n",
      "13      PEMS07   DAY      W1    0,1,2,3   -6.013226\n",
      "14      PEMS07  WEEK      W1    0,1,2,3   -5.687293\n",
      "15      PEMS07  HOUR      W2    0,1,2,3    1.048654\n",
      "16      PEMS07   DAY      W2    0,1,2,3    3.585262\n",
      "17      PEMS07  WEEK      W2    0,1,2,3   -1.610770\n",
      "18      PEMS08  HOUR      W1    0,1,2,3    0.447928\n",
      "19      PEMS08   DAY      W1    0,1,2,3    1.433371\n",
      "20      PEMS08  WEEK      W1    0,1,2,3    1.030235\n",
      "21      PEMS08  HOUR      W2    0,1,2,3    0.395670\n",
      "22      PEMS08   DAY      W2    0,1,2,3    2.889138\n",
      "23      PEMS08  WEEK      W2    0,1,2,3    1.187010\n"
     ]
    }
   ],
   "source": [
    "print('modulus - blis for skip 3')\n",
    "df_blis = df_traffic[df_traffic['scattering_type'] == 'blis'].copy() \n",
    "df_modulus = df_traffic[df_traffic['scattering_type'] == 'modulus'].copy()\n",
    "\n",
    "# Merge on shared columns and calculate difference\n",
    "merged = pd.merge(df_blis, df_modulus, on=['sub_dataset', 'label', 'wavelet', 'layer_list'], suffixes=('_blis', '_modulus'))\n",
    "merged['difference'] = merged['score_modulus'] - merged['score_blis']\n",
    "\n",
    "print(merged[['sub_dataset', 'label', 'wavelet', 'layer_list', 'difference']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Classifiers Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scattering_type</th>\n",
       "      <th>sub_dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "      <th>stdev</th>\n",
       "      <th>ncomp</th>\n",
       "      <th>task</th>\n",
       "      <th>pca_var</th>\n",
       "      <th>moment_list</th>\n",
       "      <th>layer_list</th>\n",
       "      <th>wavelet_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blis</td>\n",
       "      <td>0</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.114095</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blis</td>\n",
       "      <td>0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.623077</td>\n",
       "      <td>0.167473</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blis</td>\n",
       "      <td>0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.127794</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blis</td>\n",
       "      <td>0</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.715385</td>\n",
       "      <td>0.082849</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blis</td>\n",
       "      <td>1</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.122595</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>modulus</td>\n",
       "      <td>153</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.507692</td>\n",
       "      <td>0.056527</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>W2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>modulus</td>\n",
       "      <td>154</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.638462</td>\n",
       "      <td>0.086346</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>W2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>modulus</td>\n",
       "      <td>154</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.130995</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>W2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>modulus</td>\n",
       "      <td>154</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.018842</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>W2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>modulus</td>\n",
       "      <td>154</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.107141</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>EMOTION3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0,1,2</td>\n",
       "      <td>W2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2480 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     scattering_type  sub_dataset model     score     stdev  ncomp      task  \\\n",
       "0               blis            0   XGB  0.576923  0.114095   -1.0  EMOTION3   \n",
       "1               blis            0    RF  0.623077  0.167473   -1.0  EMOTION3   \n",
       "2               blis            0   SVC  0.600000  0.127794   -1.0  EMOTION3   \n",
       "3               blis            0    LR  0.715385  0.082849   -1.0  EMOTION3   \n",
       "4               blis            1   XGB  0.646154  0.122595   -1.0  EMOTION3   \n",
       "...              ...          ...   ...       ...       ...    ...       ...   \n",
       "2475         modulus          153    LR  0.507692  0.056527   -1.0  EMOTION3   \n",
       "2476         modulus          154   XGB  0.638462  0.086346   -1.0  EMOTION3   \n",
       "2477         modulus          154    RF  0.615385  0.130995   -1.0  EMOTION3   \n",
       "2478         modulus          154   SVC  0.523077  0.018842   -1.0  EMOTION3   \n",
       "2479         modulus          154    LR  0.492308  0.107141   -1.0  EMOTION3   \n",
       "\n",
       "      pca_var  moment_list layer_list wavelet_type  \n",
       "0         1.0            1          3           W1  \n",
       "1         1.0            1          3           W1  \n",
       "2         1.0            1          3           W1  \n",
       "3         1.0            1          3           W1  \n",
       "4         1.0            1          3           W1  \n",
       "...       ...          ...        ...          ...  \n",
       "2475      1.0            1      0,1,2           W2  \n",
       "2476      1.0            1      0,1,2           W2  \n",
       "2477      1.0            1      0,1,2           W2  \n",
       "2478      1.0            1      0,1,2           W2  \n",
       "2479      1.0            1      0,1,2           W2  \n",
       "\n",
       "[2480 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_DIR = 'results_shallow'\n",
    "\n",
    "# read in and process the partly_cloudy datasets \n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelet_types = ['W1', 'W2']\n",
    "\n",
    "partly_cloudy_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'partly_cloudy_smoothed_{scattering_type}_{wavelet_type}_EMOTION3_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type\n",
    "        partly_cloudy_shallow.append(df) \n",
    "\n",
    "partly_cloudy_shallow = pd.concat(partly_cloudy_shallow, axis = 0).reset_index(drop=True)\n",
    "partly_cloudy_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partly cloudy scattering shallow\n",
      "   scattering_type model wavelet_type           score\n",
      "0             blis    LR           W1  $62.4 \\pm 5.4$\n",
      "1             blis    LR           W2  $65.9 \\pm 5.2$\n",
      "2             blis    RF           W1  $61.5 \\pm 5.2$\n",
      "3             blis    RF           W2  $63.0 \\pm 4.5$\n",
      "4             blis   SVC           W1  $56.2 \\pm 5.3$\n",
      "5             blis   SVC           W2  $59.0 \\pm 5.0$\n",
      "6             blis   XGB           W1  $61.1 \\pm 5.7$\n",
      "7             blis   XGB           W2  $62.8 \\pm 5.1$\n",
      "8          modulus    LR           W1  $51.2 \\pm 5.8$\n",
      "9          modulus    LR           W2  $53.1 \\pm 5.9$\n",
      "10         modulus    RF           W1  $56.1 \\pm 6.0$\n",
      "11         modulus    RF           W2  $58.8 \\pm 5.5$\n",
      "12         modulus   SVC           W1  $51.5 \\pm 5.9$\n",
      "13         modulus   SVC           W2  $54.2 \\pm 6.1$\n",
      "14         modulus   XGB           W1  $56.2 \\pm 6.0$\n",
      "15         modulus   XGB           W2  $58.3 \\pm 5.8$\n"
     ]
    }
   ],
   "source": [
    "print('partly cloudy scattering shallow')\n",
    "grouped = partly_cloudy_shallow.groupby(['scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic scattering shallow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scattering_type</th>\n",
       "      <th>sub_dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>score</th>\n",
       "      <th>stdev</th>\n",
       "      <th>ncomp</th>\n",
       "      <th>pca_var</th>\n",
       "      <th>layer_list</th>\n",
       "      <th>wavelet_type</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blis</td>\n",
       "      <td>camel_pm_0</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "      <td>same mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blis</td>\n",
       "      <td>camel_pm_0</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.986667</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "      <td>same mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blis</td>\n",
       "      <td>camel_pm_0</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.018257</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "      <td>same mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blis</td>\n",
       "      <td>camel_pm_0</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "      <td>same mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blis</td>\n",
       "      <td>camel_pm_1</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.018257</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>W1</td>\n",
       "      <td>same mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>modulus</td>\n",
       "      <td>gaussian_pm_3</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.043970</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1,2</td>\n",
       "      <td>W2</td>\n",
       "      <td>different mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>modulus</td>\n",
       "      <td>gaussian_pm_4</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.786667</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1,2</td>\n",
       "      <td>W2</td>\n",
       "      <td>different mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>modulus</td>\n",
       "      <td>gaussian_pm_4</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.703333</td>\n",
       "      <td>0.038586</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1,2</td>\n",
       "      <td>W2</td>\n",
       "      <td>different mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>modulus</td>\n",
       "      <td>gaussian_pm_4</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.053125</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1,2</td>\n",
       "      <td>W2</td>\n",
       "      <td>different mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>modulus</td>\n",
       "      <td>gaussian_pm_4</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.846667</td>\n",
       "      <td>0.043970</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1,2</td>\n",
       "      <td>W2</td>\n",
       "      <td>different mu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    scattering_type    sub_dataset model     score     stdev  ncomp  pca_var  \\\n",
       "0              blis     camel_pm_0   XGB  0.986667  0.006667   -1.0      1.0   \n",
       "1              blis     camel_pm_0    RF  0.986667  0.012472   -1.0      1.0   \n",
       "2              blis     camel_pm_0   SVC  0.983333  0.018257   -1.0      1.0   \n",
       "3              blis     camel_pm_0    LR  0.966667  0.014907   -1.0      1.0   \n",
       "4              blis     camel_pm_1   XGB  0.983333  0.018257   -1.0      1.0   \n",
       "..              ...            ...   ...       ...       ...    ...      ...   \n",
       "155         modulus  gaussian_pm_3    LR  0.846667  0.043970   -1.0      1.0   \n",
       "156         modulus  gaussian_pm_4   XGB  0.786667  0.035590   -1.0      1.0   \n",
       "157         modulus  gaussian_pm_4    RF  0.703333  0.038586   -1.0      1.0   \n",
       "158         modulus  gaussian_pm_4   SVC  0.853333  0.053125   -1.0      1.0   \n",
       "159         modulus  gaussian_pm_4    LR  0.846667  0.043970   -1.0      1.0   \n",
       "\n",
       "    layer_list wavelet_type          task  \n",
       "0            3           W1       same mu  \n",
       "1            3           W1       same mu  \n",
       "2            3           W1       same mu  \n",
       "3            3           W1       same mu  \n",
       "4            3           W1       same mu  \n",
       "..         ...          ...           ...  \n",
       "155        1,2           W2  different mu  \n",
       "156        1,2           W2  different mu  \n",
       "157        1,2           W2  different mu  \n",
       "158        1,2           W2  different mu  \n",
       "159        1,2           W2  different mu  \n",
       "\n",
       "[160 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('synthetic scattering shallow')\n",
    "# read in and process the partly_cloudy datasets \n",
    "scattering_types = ['blis', 'modulus']\n",
    "wavelet_types = ['W1', 'W2']\n",
    "\n",
    "synthetic_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'synthetic_results_{scattering_type}_{wavelet_type}_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type\n",
    "        # convert sub_dataset to a task label\n",
    "        df['task'] = np.where(df['sub_dataset'].str.startswith('camel_pm'), 'same mu',\n",
    "                            np.where(df['sub_dataset'].str.startswith('gaussian_pm'), 'different mu', np.nan))\n",
    "        synthetic_shallow.append(df) \n",
    "\n",
    "synthetic_shallow = pd.concat(synthetic_shallow, axis = 0).reset_index(drop=True)\n",
    "synthetic_shallow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synthetic scattering shallow\n",
      "            task scattering_type model wavelet_type            score\n",
      "0   different mu            blis    LR           W1  $100.0 \\pm 0.0$\n",
      "1   different mu            blis    LR           W2  $100.0 \\pm 0.0$\n",
      "2   different mu            blis    RF           W1   $99.2 \\pm 0.4$\n",
      "3   different mu            blis    RF           W2   $99.4 \\pm 0.1$\n",
      "4   different mu            blis   SVC           W1   $99.4 \\pm 0.1$\n",
      "5   different mu            blis   SVC           W2  $100.0 \\pm 0.0$\n",
      "6   different mu            blis   XGB           W1   $99.5 \\pm 0.3$\n",
      "7   different mu            blis   XGB           W2   $99.3 \\pm 0.0$\n",
      "8   different mu         modulus    LR           W1   $97.7 \\pm 0.7$\n",
      "9   different mu         modulus    LR           W2   $86.9 \\pm 4.9$\n",
      "10  different mu         modulus    RF           W1   $95.9 \\pm 1.6$\n",
      "11  different mu         modulus    RF           W2   $73.4 \\pm 8.0$\n",
      "12  different mu         modulus   SVC           W1   $98.1 \\pm 0.9$\n",
      "13  different mu         modulus   SVC           W2   $87.5 \\pm 4.9$\n",
      "14  different mu         modulus   XGB           W1   $95.2 \\pm 1.9$\n",
      "15  different mu         modulus   XGB           W2   $81.9 \\pm 7.2$\n",
      "16       same mu            blis    LR           W1   $98.5 \\pm 1.0$\n",
      "17       same mu            blis    LR           W2   $98.8 \\pm 0.4$\n",
      "18       same mu            blis    RF           W1   $97.7 \\pm 0.6$\n",
      "19       same mu            blis    RF           W2   $97.1 \\pm 0.7$\n",
      "20       same mu            blis   SVC           W1   $95.7 \\pm 1.5$\n",
      "21       same mu            blis   SVC           W2   $95.5 \\pm 1.9$\n",
      "22       same mu            blis   XGB           W1   $98.4 \\pm 0.1$\n",
      "23       same mu            blis   XGB           W2   $97.7 \\pm 0.7$\n",
      "24       same mu         modulus    LR           W1   $96.1 \\pm 1.0$\n",
      "25       same mu         modulus    LR           W2   $95.3 \\pm 1.5$\n",
      "26       same mu         modulus    RF           W1   $94.5 \\pm 1.6$\n",
      "27       same mu         modulus    RF           W2   $94.1 \\pm 1.6$\n",
      "28       same mu         modulus   SVC           W1   $95.0 \\pm 1.5$\n",
      "29       same mu         modulus   SVC           W2   $93.5 \\pm 1.8$\n",
      "30       same mu         modulus   XGB           W1   $93.1 \\pm 2.5$\n",
      "31       same mu         modulus   XGB           W2   $94.7 \\pm 1.5$\n"
     ]
    }
   ],
   "source": [
    "print('synthetic scattering shallow')\n",
    "grouped = synthetic_shallow.groupby(['task','scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic scattering PEMS03 shallow\n",
      "    task scattering_type model wavelet_type           score\n",
      "0    DAY            blis    LR           W1  $37.0 \\pm nan$\n",
      "1    DAY            blis    LR           W2  $42.2 \\pm nan$\n",
      "2    DAY            blis    RF           W1  $52.3 \\pm nan$\n",
      "3    DAY            blis    RF           W2  $53.4 \\pm nan$\n",
      "4    DAY            blis   SVC           W1  $35.1 \\pm nan$\n",
      "5    DAY            blis   SVC           W2  $35.9 \\pm nan$\n",
      "6    DAY            blis   XGB           W1  $54.0 \\pm nan$\n",
      "7    DAY            blis   XGB           W2  $56.3 \\pm nan$\n",
      "8    DAY         modulus    LR           W1  $33.1 \\pm nan$\n",
      "9    DAY         modulus    LR           W2  $33.2 \\pm nan$\n",
      "10   DAY         modulus    RF           W1  $44.8 \\pm nan$\n",
      "11   DAY         modulus    RF           W2  $46.6 \\pm nan$\n",
      "12   DAY         modulus   SVC           W1  $28.3 \\pm nan$\n",
      "13   DAY         modulus   SVC           W2  $30.5 \\pm nan$\n",
      "14   DAY         modulus   XGB           W1  $42.8 \\pm nan$\n",
      "15   DAY         modulus   XGB           W2  $44.6 \\pm nan$\n",
      "16  HOUR            blis    LR           W1  $49.0 \\pm nan$\n",
      "17  HOUR            blis    LR           W2  $53.0 \\pm nan$\n",
      "18  HOUR            blis    RF           W1  $63.4 \\pm nan$\n",
      "19  HOUR            blis    RF           W2  $63.5 \\pm nan$\n",
      "20  HOUR            blis   SVC           W1  $49.1 \\pm nan$\n",
      "21  HOUR            blis   SVC           W2  $49.5 \\pm nan$\n",
      "22  HOUR            blis   XGB           W1  $68.8 \\pm nan$\n",
      "23  HOUR            blis   XGB           W2  $69.2 \\pm nan$\n",
      "24  HOUR         modulus    LR           W1  $42.3 \\pm nan$\n",
      "25  HOUR         modulus    LR           W2  $46.0 \\pm nan$\n",
      "26  HOUR         modulus    RF           W1  $56.0 \\pm nan$\n",
      "27  HOUR         modulus    RF           W2  $57.9 \\pm nan$\n",
      "28  HOUR         modulus   SVC           W1  $43.5 \\pm nan$\n",
      "29  HOUR         modulus   SVC           W2  $46.5 \\pm nan$\n",
      "30  HOUR         modulus   XGB           W1  $56.7 \\pm nan$\n",
      "31  HOUR         modulus   XGB           W2  $59.5 \\pm nan$\n",
      "32  WEEK            blis    LR           W1  $43.4 \\pm nan$\n",
      "33  WEEK            blis    LR           W2  $46.7 \\pm nan$\n",
      "34  WEEK            blis    RF           W1  $52.9 \\pm nan$\n",
      "35  WEEK            blis    RF           W2  $55.7 \\pm nan$\n",
      "36  WEEK            blis   SVC           W1  $37.9 \\pm nan$\n",
      "37  WEEK            blis   SVC           W2  $41.6 \\pm nan$\n",
      "38  WEEK            blis   XGB           W1  $52.6 \\pm nan$\n",
      "39  WEEK            blis   XGB           W2  $56.2 \\pm nan$\n",
      "40  WEEK         modulus    LR           W1  $37.8 \\pm nan$\n",
      "41  WEEK         modulus    LR           W2  $39.1 \\pm nan$\n",
      "42  WEEK         modulus    RF           W1  $43.9 \\pm nan$\n",
      "43  WEEK         modulus    RF           W2  $48.4 \\pm nan$\n",
      "44  WEEK         modulus   SVC           W1  $32.6 \\pm nan$\n",
      "45  WEEK         modulus   SVC           W2  $36.1 \\pm nan$\n",
      "46  WEEK         modulus   XGB           W1  $41.6 \\pm nan$\n",
      "47  WEEK         modulus   XGB           W2  $45.7 \\pm nan$\n"
     ]
    }
   ],
   "source": [
    "# do the same for the traffic dataset\n",
    "DATASET = 'PEMS03'\n",
    "\n",
    "traffic_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df)\n",
    "\n",
    "traffic_shallow = pd.concat(traffic_shallow, axis = 0).reset_index(drop=True)\n",
    "\n",
    "print(f'traffic scattering {DATASET} shallow')\n",
    "grouped = traffic_shallow.groupby(['task','scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic scattering PEMS07 shallow\n",
      "    task scattering_type model wavelet_type           score\n",
      "0    DAY            blis    LR           W1  $46.2 \\pm nan$\n",
      "1    DAY            blis    LR           W2  $41.0 \\pm nan$\n",
      "2    DAY            blis    RF           W1  $67.7 \\pm nan$\n",
      "3    DAY            blis    RF           W2  $62.7 \\pm nan$\n",
      "4    DAY            blis   SVC           W1  $53.7 \\pm nan$\n",
      "5    DAY            blis   SVC           W2  $41.8 \\pm nan$\n",
      "6    DAY            blis   XGB           W1  $74.7 \\pm nan$\n",
      "7    DAY            blis   XGB           W2  $66.5 \\pm nan$\n",
      "8    DAY         modulus    LR           W1  $33.3 \\pm nan$\n",
      "9    DAY         modulus    LR           W2  $29.9 \\pm nan$\n",
      "10   DAY         modulus    RF           W1  $51.9 \\pm nan$\n",
      "11   DAY         modulus    RF           W2  $53.4 \\pm nan$\n",
      "12   DAY         modulus   SVC           W1  $35.5 \\pm nan$\n",
      "13   DAY         modulus   SVC           W2  $34.7 \\pm nan$\n",
      "14   DAY         modulus   XGB           W1  $50.2 \\pm nan$\n",
      "15   DAY         modulus   XGB           W2  $50.9 \\pm nan$\n",
      "16  HOUR            blis    LR           W1  $47.3 \\pm nan$\n",
      "17  HOUR            blis    LR           W2  $43.4 \\pm nan$\n",
      "18  HOUR            blis    RF           W1  $60.7 \\pm nan$\n",
      "19  HOUR            blis    RF           W2  $57.4 \\pm nan$\n",
      "20  HOUR            blis   SVC           W1  $51.1 \\pm nan$\n",
      "21  HOUR            blis   SVC           W2  $43.2 \\pm nan$\n",
      "22  HOUR            blis   XGB           W1  $68.6 \\pm nan$\n",
      "23  HOUR            blis   XGB           W2  $64.8 \\pm nan$\n",
      "24  HOUR         modulus    LR           W1  $36.7 \\pm nan$\n",
      "25  HOUR         modulus    LR           W2  $36.7 \\pm nan$\n",
      "26  HOUR         modulus    RF           W1  $53.5 \\pm nan$\n",
      "27  HOUR         modulus    RF           W2  $52.7 \\pm nan$\n",
      "28  HOUR         modulus   SVC           W1  $39.7 \\pm nan$\n",
      "29  HOUR         modulus   SVC           W2  $40.6 \\pm nan$\n",
      "30  HOUR         modulus   XGB           W1  $53.2 \\pm nan$\n",
      "31  HOUR         modulus   XGB           W2  $54.1 \\pm nan$\n",
      "32  WEEK            blis    LR           W1  $54.5 \\pm nan$\n",
      "33  WEEK            blis    LR           W2  $51.3 \\pm nan$\n",
      "34  WEEK            blis    RF           W1  $71.9 \\pm nan$\n",
      "35  WEEK            blis    RF           W2  $67.6 \\pm nan$\n",
      "36  WEEK            blis   SVC           W1  $56.7 \\pm nan$\n",
      "37  WEEK            blis   SVC           W2  $46.9 \\pm nan$\n",
      "38  WEEK            blis   XGB           W1  $75.3 \\pm nan$\n",
      "39  WEEK            blis   XGB           W2  $69.1 \\pm nan$\n",
      "40  WEEK         modulus    LR           W1  $39.4 \\pm nan$\n",
      "41  WEEK         modulus    LR           W2  $40.4 \\pm nan$\n",
      "42  WEEK         modulus    RF           W1  $52.6 \\pm nan$\n",
      "43  WEEK         modulus    RF           W2  $56.5 \\pm nan$\n",
      "44  WEEK         modulus   SVC           W1  $38.7 \\pm nan$\n",
      "45  WEEK         modulus   SVC           W2  $42.0 \\pm nan$\n",
      "46  WEEK         modulus   XGB           W1  $49.1 \\pm nan$\n",
      "47  WEEK         modulus   XGB           W2  $52.6 \\pm nan$\n"
     ]
    }
   ],
   "source": [
    "# do the same for the traffic dataset\n",
    "DATASET = 'PEMS07'\n",
    "\n",
    "traffic_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df)\n",
    "\n",
    "traffic_shallow = pd.concat(traffic_shallow, axis = 0).reset_index(drop=True)\n",
    "\n",
    "print(f'traffic scattering {DATASET} shallow')\n",
    "grouped = traffic_shallow.groupby(['task','scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic scattering PEMS04 shallow\n",
      "    task scattering_type model wavelet_type           score\n",
      "0    DAY            blis    LR           W1  $71.4 \\pm nan$\n",
      "1    DAY            blis    LR           W2  $69.4 \\pm nan$\n",
      "2    DAY            blis   MLP           W1  $87.8 \\pm nan$\n",
      "3    DAY            blis   MLP           W2  $91.9 \\pm nan$\n",
      "4    DAY            blis    RF           W1  $89.8 \\pm nan$\n",
      "5    DAY            blis    RF           W2  $88.5 \\pm nan$\n",
      "6    DAY            blis   SVC           W1  $75.5 \\pm nan$\n",
      "7    DAY            blis   SVC           W2  $73.5 \\pm nan$\n",
      "8    DAY            blis   XGB           W1  $93.9 \\pm nan$\n",
      "9    DAY            blis   XGB           W2  $92.8 \\pm nan$\n",
      "10   DAY         modulus    LR           W1  $47.7 \\pm nan$\n",
      "11   DAY         modulus    LR           W2  $49.8 \\pm nan$\n",
      "12   DAY         modulus   MLP           W1  $83.2 \\pm nan$\n",
      "13   DAY         modulus   MLP           W2  $85.9 \\pm nan$\n",
      "14   DAY         modulus    RF           W1  $79.0 \\pm nan$\n",
      "15   DAY         modulus    RF           W2  $82.9 \\pm nan$\n",
      "16   DAY         modulus   SVC           W1  $55.9 \\pm nan$\n",
      "17   DAY         modulus   SVC           W2  $62.9 \\pm nan$\n",
      "18   DAY         modulus   XGB           W1  $79.1 \\pm nan$\n",
      "19   DAY         modulus   XGB           W2  $82.9 \\pm nan$\n",
      "20  HOUR            blis    LR           W1  $74.7 \\pm nan$\n",
      "21  HOUR            blis    LR           W2  $71.5 \\pm nan$\n",
      "22  HOUR            blis   MLP           W1  $82.9 \\pm nan$\n",
      "23  HOUR            blis   MLP           W2  $84.2 \\pm nan$\n",
      "24  HOUR            blis    RF           W1  $82.4 \\pm nan$\n",
      "25  HOUR            blis    RF           W2  $80.7 \\pm nan$\n",
      "26  HOUR            blis   SVC           W1  $71.9 \\pm nan$\n",
      "27  HOUR            blis   SVC           W2  $69.5 \\pm nan$\n",
      "28  HOUR            blis   XGB           W1  $86.4 \\pm nan$\n",
      "29  HOUR            blis   XGB           W2  $86.1 \\pm nan$\n",
      "30  HOUR         modulus    LR           W1  $58.8 \\pm nan$\n",
      "31  HOUR         modulus    LR           W2  $63.3 \\pm nan$\n",
      "32  HOUR         modulus   MLP           W1  $78.5 \\pm nan$\n",
      "33  HOUR         modulus   MLP           W2  $81.2 \\pm nan$\n",
      "34  HOUR         modulus    RF           W1  $76.8 \\pm nan$\n",
      "35  HOUR         modulus    RF           W2  $78.4 \\pm nan$\n",
      "36  HOUR         modulus   SVC           W1  $60.0 \\pm nan$\n",
      "37  HOUR         modulus   SVC           W2  $64.2 \\pm nan$\n",
      "38  HOUR         modulus   XGB           W1  $81.3 \\pm nan$\n",
      "39  HOUR         modulus   XGB           W2  $82.6 \\pm nan$\n",
      "40  WEEK            blis    LR           W1  $69.5 \\pm nan$\n",
      "41  WEEK            blis    LR           W2  $68.2 \\pm nan$\n",
      "42  WEEK            blis   MLP           W1  $91.1 \\pm nan$\n",
      "43  WEEK            blis   MLP           W2  $92.3 \\pm nan$\n",
      "44  WEEK            blis    RF           W1  $90.9 \\pm nan$\n",
      "45  WEEK            blis    RF           W2  $89.5 \\pm nan$\n",
      "46  WEEK            blis   SVC           W1  $77.0 \\pm nan$\n",
      "47  WEEK            blis   SVC           W2  $75.6 \\pm nan$\n",
      "48  WEEK            blis   XGB           W1  $93.6 \\pm nan$\n",
      "49  WEEK            blis   XGB           W2  $92.9 \\pm nan$\n",
      "50  WEEK         modulus    LR           W1  $44.7 \\pm nan$\n",
      "51  WEEK         modulus    LR           W2  $47.6 \\pm nan$\n",
      "52  WEEK         modulus   MLP           W1  $83.8 \\pm nan$\n",
      "53  WEEK         modulus   MLP           W2  $86.4 \\pm nan$\n",
      "54  WEEK         modulus    RF           W1  $79.3 \\pm nan$\n",
      "55  WEEK         modulus    RF           W2  $82.6 \\pm nan$\n",
      "56  WEEK         modulus   SVC           W1  $55.6 \\pm nan$\n",
      "57  WEEK         modulus   SVC           W2  $61.0 \\pm nan$\n",
      "58  WEEK         modulus   XGB           W1  $75.8 \\pm nan$\n",
      "59  WEEK         modulus   XGB           W2  $79.8 \\pm nan$\n"
     ]
    }
   ],
   "source": [
    "# do the same for the traffic dataset\n",
    "DATASET = 'PEMS04'\n",
    "\n",
    "# note that the MLP results needed to be re-run, so I will include them here too\n",
    "\n",
    "traffic_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df)\n",
    "\n",
    "        df_MLP = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_MLP.csv'))\n",
    "        df_MLP[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df_MLP)\n",
    "\n",
    "\n",
    "\n",
    "traffic_shallow = pd.concat(traffic_shallow, axis = 0).reset_index(drop=True)\n",
    "\n",
    "print(f'traffic scattering {DATASET} shallow')\n",
    "grouped = traffic_shallow.groupby(['task','scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic scattering PEMS08 shallow\n",
      "    task scattering_type model wavelet_type           score\n",
      "0    DAY            blis    LR           W1  $76.5 \\pm nan$\n",
      "1    DAY            blis    LR           W2  $80.2 \\pm nan$\n",
      "2    DAY            blis   MLP           W1  $92.9 \\pm nan$\n",
      "3    DAY            blis   MLP           W2  $94.9 \\pm nan$\n",
      "4    DAY            blis    RF           W1  $92.7 \\pm nan$\n",
      "5    DAY            blis    RF           W2  $93.5 \\pm nan$\n",
      "6    DAY            blis   SVC           W1  $85.2 \\pm nan$\n",
      "7    DAY            blis   SVC           W2  $87.4 \\pm nan$\n",
      "8    DAY            blis   XGB           W1  $95.1 \\pm nan$\n",
      "9    DAY            blis   XGB           W2  $96.0 \\pm nan$\n",
      "10   DAY         modulus    LR           W1  $56.1 \\pm nan$\n",
      "11   DAY         modulus    LR           W2  $60.1 \\pm nan$\n",
      "12   DAY         modulus   MLP           W1  $89.9 \\pm nan$\n",
      "13   DAY         modulus   MLP           W2  $92.0 \\pm nan$\n",
      "14   DAY         modulus    RF           W1  $86.0 \\pm nan$\n",
      "15   DAY         modulus    RF           W2  $88.1 \\pm nan$\n",
      "16   DAY         modulus   SVC           W1  $71.5 \\pm nan$\n",
      "17   DAY         modulus   SVC           W2  $76.6 \\pm nan$\n",
      "18   DAY         modulus   XGB           W1  $87.2 \\pm nan$\n",
      "19   DAY         modulus   XGB           W2  $89.6 \\pm nan$\n",
      "20  HOUR            blis    LR           W1  $69.5 \\pm nan$\n",
      "21  HOUR            blis    LR           W2  $71.9 \\pm nan$\n",
      "22  HOUR            blis   MLP           W1  $83.9 \\pm nan$\n",
      "23  HOUR            blis   MLP           W2  $85.9 \\pm nan$\n",
      "24  HOUR            blis    RF           W1  $83.7 \\pm nan$\n",
      "25  HOUR            blis    RF           W2  $83.9 \\pm nan$\n",
      "26  HOUR            blis   SVC           W1  $72.4 \\pm nan$\n",
      "27  HOUR            blis   SVC           W2  $73.8 \\pm nan$\n",
      "28  HOUR            blis   XGB           W1  $87.2 \\pm nan$\n",
      "29  HOUR            blis   XGB           W2  $87.7 \\pm nan$\n",
      "30  HOUR         modulus    LR           W1  $56.6 \\pm nan$\n",
      "31  HOUR         modulus    LR           W2  $58.6 \\pm nan$\n",
      "32  HOUR         modulus   MLP           W1  $81.0 \\pm nan$\n",
      "33  HOUR         modulus   MLP           W2  $82.2 \\pm nan$\n",
      "34  HOUR         modulus    RF           W1  $79.2 \\pm nan$\n",
      "35  HOUR         modulus    RF           W2  $80.6 \\pm nan$\n",
      "36  HOUR         modulus   SVC           W1  $63.4 \\pm nan$\n",
      "37  HOUR         modulus   SVC           W2  $66.1 \\pm nan$\n",
      "38  HOUR         modulus   XGB           W1  $81.8 \\pm nan$\n",
      "39  HOUR         modulus   XGB           W2  $83.6 \\pm nan$\n",
      "40  WEEK            blis    LR           W1  $78.1 \\pm nan$\n",
      "41  WEEK            blis    LR           W2  $81.8 \\pm nan$\n",
      "42  WEEK            blis   MLP           W1  $93.4 \\pm nan$\n",
      "43  WEEK            blis   MLP           W2  $95.6 \\pm nan$\n",
      "44  WEEK            blis    RF           W1  $91.6 \\pm nan$\n",
      "45  WEEK            blis    RF           W2  $93.6 \\pm nan$\n",
      "46  WEEK            blis   SVC           W1  $84.9 \\pm nan$\n",
      "47  WEEK            blis   SVC           W2  $89.5 \\pm nan$\n",
      "48  WEEK            blis   XGB           W1  $94.7 \\pm nan$\n",
      "49  WEEK            blis   XGB           W2  $96.1 \\pm nan$\n",
      "50  WEEK         modulus    LR           W1  $54.1 \\pm nan$\n",
      "51  WEEK         modulus    LR           W2  $57.8 \\pm nan$\n",
      "52  WEEK         modulus   MLP           W1  $89.3 \\pm nan$\n",
      "53  WEEK         modulus   MLP           W2  $90.7 \\pm nan$\n",
      "54  WEEK         modulus    RF           W1  $84.2 \\pm nan$\n",
      "55  WEEK         modulus    RF           W2  $87.9 \\pm nan$\n",
      "56  WEEK         modulus   SVC           W1  $66.2 \\pm nan$\n",
      "57  WEEK         modulus   SVC           W2  $72.0 \\pm nan$\n",
      "58  WEEK         modulus   XGB           W1  $81.9 \\pm nan$\n",
      "59  WEEK         modulus   XGB           W2  $86.1 \\pm nan$\n"
     ]
    }
   ],
   "source": [
    "# do the same for the traffic dataset\n",
    "DATASET = 'PEMS08'\n",
    "\n",
    "traffic_shallow = []\n",
    "for scattering_type in scattering_types:\n",
    "    for wavelet_type in wavelet_types:\n",
    "        df = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_shallow.csv'))\n",
    "        df[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df)\n",
    "\n",
    "        df_MLP = pd.read_csv(os.path.join(RESULTS_DIR, f'traffic_{DATASET}_{scattering_type}_{wavelet_type}_MLP.csv'))\n",
    "        df_MLP[\"wavelet_type\"] = wavelet_type \n",
    "        traffic_shallow.append(df_MLP)\n",
    "\n",
    "traffic_shallow = pd.concat(traffic_shallow, axis = 0).reset_index(drop=True)\n",
    "\n",
    "print(f'traffic scattering {DATASET} shallow')\n",
    "grouped = traffic_shallow.groupby(['task','scattering_type', 'model', 'wavelet_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuttal experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame:\n",
      "  model  hidden_dim  epochs  learning_rate task_type        dataset  \\\n",
      "0   GAT          16     100          0.001  EMOTION3  partly_cloudy   \n",
      "1   GAT          16     100          0.001  EMOTION3  partly_cloudy   \n",
      "2   GAT          16     100          0.001  EMOTION3  partly_cloudy   \n",
      "3   GAT          16     100          0.001  EMOTION3  partly_cloudy   \n",
      "4   GAT          16     100          0.001  EMOTION3  partly_cloudy   \n",
      "\n",
      "  sub_dataset      score     stdev  \n",
      "0           0  28.627451  4.401950  \n",
      "1           1  38.823529  5.736760  \n",
      "2           2  48.235294  5.628510  \n",
      "3           3  40.392157  6.746922  \n",
      "4           4  29.019608  2.286648  \n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "# first process all of the GAT data\n",
    "\n",
    "results_dir = 'run_results'\n",
    "gat_files = glob.glob(os.path.join(results_dir, '*GAT*.csv'))  # Find all files containing 'GAT'\n",
    "\n",
    "dataframes = []\n",
    "for file in gat_files:\n",
    "    df = pd.read_csv(file)  # Assuming the files are in CSV format\n",
    "    dataframes.append(df)\n",
    "\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "print(\"Combined DataFrame:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT Partly Cloudy Result: $40.6 \\pm 6.1 $\n",
      "GAT Synthetic camel Result: $96.4 \\pm 0.6 $\n",
      "GAT synthetic gaussian result: $98.6 \\pm 0.8 $\n"
     ]
    }
   ],
   "source": [
    "GAT_partly_cloudy_mean = df[df['dataset'] == 'partly_cloudy' ]['score'].mean()\n",
    "GAT_partly_cloudy_stdev = df[df['dataset'] == 'partly_cloudy' ]['score'].std()\n",
    "GAT_camel_mean = df[df['sub_dataset'].str.startswith('camel', na = False)]['score'].mean()\n",
    "GAT_camel_stdev = df[df['sub_dataset'].str.startswith('camel', na = False)]['score'].std()\n",
    "GAT_gaussian_mean = df[df['sub_dataset'].str.startswith('gaussian', na = False)]['score'].mean()\n",
    "GAT_gaussian_stdev = df[df['sub_dataset'].str.startswith('gaussian', na = False)]['score'].std()\n",
    "print(f'GAT Partly Cloudy Result: ${GAT_partly_cloudy_mean:.1f} \\pm {GAT_partly_cloudy_stdev:.1f} $')\n",
    "print(f'GAT Synthetic camel Result: ${GAT_camel_mean:.1f} \\pm {GAT_camel_stdev:.1f} $')\n",
    "print(f'GAT synthetic gaussian result: ${GAT_gaussian_mean:.1f} \\pm {GAT_gaussian_stdev:.1f} $')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>task_type</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sub_dataset</th>\n",
       "      <th>score</th>\n",
       "      <th>stdev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>DAY</td>\n",
       "      <td>traffic</td>\n",
       "      <td>PEMS07</td>\n",
       "      <td>22.187057</td>\n",
       "      <td>1.159908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>HOUR</td>\n",
       "      <td>traffic</td>\n",
       "      <td>PEMS07</td>\n",
       "      <td>33.160132</td>\n",
       "      <td>1.252239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>GAT</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>WEEK</td>\n",
       "      <td>traffic</td>\n",
       "      <td>PEMS07</td>\n",
       "      <td>36.506849</td>\n",
       "      <td>0.857303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    model  hidden_dim  epochs  learning_rate task_type  dataset sub_dataset  \\\n",
       "171   GAT          16     100          0.001       DAY  traffic      PEMS07   \n",
       "172   GAT          16     100          0.001      HOUR  traffic      PEMS07   \n",
       "173   GAT          16     100          0.001      WEEK  traffic      PEMS07   \n",
       "\n",
       "         score     stdev  \n",
       "171  22.187057  1.159908  \n",
       "172  33.160132  1.252239  \n",
       "173  36.506849  0.857303  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GAT traffic result\n",
    "df[df['sub_dataset'] == 'PEMS07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      model    dataset    sub_dataset  task_type           score\n",
      "0   ChebNet  synthetic     camel_pm_0  PLUSMINUS  $98.3 \\pm nan$\n",
      "1   ChebNet  synthetic     camel_pm_1  PLUSMINUS  $97.2 \\pm nan$\n",
      "2   ChebNet  synthetic     camel_pm_2  PLUSMINUS  $96.8 \\pm nan$\n",
      "3   ChebNet  synthetic     camel_pm_3  PLUSMINUS  $97.5 \\pm nan$\n",
      "4   ChebNet  synthetic     camel_pm_4  PLUSMINUS  $96.7 \\pm nan$\n",
      "5   ChebNet  synthetic  gaussian_pm_0  PLUSMINUS  $99.5 \\pm nan$\n",
      "6   ChebNet  synthetic  gaussian_pm_1  PLUSMINUS  $98.8 \\pm nan$\n",
      "7   ChebNet  synthetic  gaussian_pm_2  PLUSMINUS  $99.2 \\pm nan$\n",
      "8   ChebNet  synthetic  gaussian_pm_3  PLUSMINUS  $98.8 \\pm nan$\n",
      "9   ChebNet  synthetic  gaussian_pm_4  PLUSMINUS  $99.0 \\pm nan$\n",
      "10  ChebNet    traffic         PEMS03        DAY  $24.6 \\pm nan$\n",
      "11  ChebNet    traffic         PEMS03       HOUR  $50.9 \\pm nan$\n",
      "12  ChebNet    traffic         PEMS03       WEEK  $56.1 \\pm nan$\n",
      "13  ChebNet    traffic         PEMS04        DAY  $50.5 \\pm nan$\n",
      "14  ChebNet    traffic         PEMS04       HOUR  $44.8 \\pm nan$\n",
      "15  ChebNet    traffic         PEMS04       WEEK  $59.8 \\pm nan$\n",
      "16  ChebNet    traffic         PEMS07        DAY  $50.9 \\pm nan$\n",
      "17  ChebNet    traffic         PEMS07       HOUR  $27.6 \\pm nan$\n",
      "18  ChebNet    traffic         PEMS07       WEEK  $64.1 \\pm nan$\n",
      "19  ChebNet    traffic         PEMS08        DAY  $41.3 \\pm nan$\n",
      "20  ChebNet    traffic         PEMS08       HOUR  $42.5 \\pm nan$\n",
      "21  ChebNet    traffic         PEMS08       WEEK  $87.8 \\pm nan$\n",
      "22   GNNML1  synthetic     camel_pm_0  PLUSMINUS  $98.5 \\pm nan$\n",
      "23   GNNML1  synthetic     camel_pm_1  PLUSMINUS  $98.8 \\pm nan$\n",
      "24   GNNML1  synthetic     camel_pm_2  PLUSMINUS  $98.0 \\pm nan$\n",
      "25   GNNML1  synthetic     camel_pm_3  PLUSMINUS  $98.0 \\pm nan$\n",
      "26   GNNML1  synthetic     camel_pm_4  PLUSMINUS  $98.2 \\pm nan$\n",
      "27   GNNML1  synthetic  gaussian_pm_0  PLUSMINUS  $99.3 \\pm nan$\n",
      "28   GNNML1  synthetic  gaussian_pm_1  PLUSMINUS  $99.5 \\pm nan$\n",
      "29   GNNML1  synthetic  gaussian_pm_2  PLUSMINUS  $99.0 \\pm nan$\n",
      "30   GNNML1  synthetic  gaussian_pm_3  PLUSMINUS  $99.3 \\pm nan$\n",
      "31   GNNML1  synthetic  gaussian_pm_4  PLUSMINUS  $99.2 \\pm nan$\n",
      "32   GNNML1    traffic         PEMS03        DAY  $15.2 \\pm nan$\n",
      "33   GNNML1    traffic         PEMS03       HOUR   $6.8 \\pm nan$\n",
      "34   GNNML1    traffic         PEMS03       WEEK  $30.3 \\pm nan$\n",
      "35   GNNML1    traffic         PEMS04        DAY  $15.8 \\pm nan$\n",
      "36   GNNML1    traffic         PEMS04       HOUR   $5.4 \\pm nan$\n",
      "37   GNNML1    traffic         PEMS04       WEEK  $27.1 \\pm nan$\n",
      "38   GNNML1    traffic         PEMS07        DAY  $17.5 \\pm nan$\n",
      "39   GNNML1    traffic         PEMS07       HOUR   $8.0 \\pm nan$\n",
      "40   GNNML1    traffic         PEMS07       WEEK  $28.3 \\pm nan$\n",
      "41   GNNML1    traffic         PEMS08        DAY  $14.5 \\pm nan$\n",
      "42   GNNML1    traffic         PEMS08       HOUR   $4.5 \\pm nan$\n",
      "43   GNNML1    traffic         PEMS08       WEEK  $28.9 \\pm nan$\n"
     ]
    }
   ],
   "source": [
    "# now look at how well the new models perform on the datasets:\n",
    "\n",
    "new_model_files = glob.glob(os.path.join(results_dir, '*multi-model*'))\n",
    "\n",
    "df_list = []\n",
    "for f in new_model_files:\n",
    "    df = pd.read_csv(f)\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index = True)\n",
    "df.head()\n",
    "\n",
    "grouped = df.groupby(['model', 'dataset', 'sub_dataset', 'task_type'])['score']\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score']).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score']).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scattering_type sub_dataset model     score     stdev  ncomp      task  \\\n",
      "0            blis           0   MLP  0.715385  0.115641   -1.0  EMOTION3   \n",
      "1            blis           1   MLP  0.661538  0.117670   -1.0  EMOTION3   \n",
      "2            blis           2   MLP  0.723077  0.074580   -1.0  EMOTION3   \n",
      "3            blis           3   MLP  0.684615  0.044853   -1.0  EMOTION3   \n",
      "4            blis           4   MLP  0.723077  0.085658   -1.0  EMOTION3   \n",
      "\n",
      "   pca_var  moment_list layer_list wavelet_type        dataset  largest_scale  \n",
      "0        1            1      0,1,2           W1  partly_cloudy              4  \n",
      "1        1            1      0,1,2           W1  partly_cloudy              4  \n",
      "2        1            1      0,1,2           W1  partly_cloudy              4  \n",
      "3        1            1      0,1,2           W1  partly_cloudy              4  \n",
      "4        1            1      0,1,2           W1  partly_cloudy              4  \n",
      "  model scattering_type  largest_scale layer_list  moment_list wavelet_type  \\\n",
      "0   MLP            blis              4          3            1           W1   \n",
      "1   MLP            blis              4          3            1           W2   \n",
      "2   MLP            blis              4      0,1,2            1           W1   \n",
      "3   MLP            blis              4      0,1,2            1           W2   \n",
      "4   MLP         modulus              4          3            1           W1   \n",
      "5   MLP         modulus              4          3            1           W2   \n",
      "6   MLP         modulus              4      0,1,2            1           W1   \n",
      "7   MLP         modulus              4      0,1,2            1           W2   \n",
      "\n",
      "            score  \n",
      "0  $66.4 \\pm 4.3$  \n",
      "1  $68.1 \\pm 3.9$  \n",
      "2  $66.7 \\pm 4.2$  \n",
      "3  $67.9 \\pm 4.3$  \n",
      "4  $56.6 \\pm 6.0$  \n",
      "5  $61.5 \\pm 5.2$  \n",
      "6  $60.7 \\pm 5.0$  \n",
      "7  $62.9 \\pm 4.9$  \n"
     ]
    }
   ],
   "source": [
    "# check the shallow classifier results\n",
    "\n",
    "# partly cloudy and synthetic first\n",
    "MLP_results_files = glob.glob(os.path.join(results_dir, '*W12*'))\n",
    "\n",
    "df_list = []\n",
    "for f in MLP_results_files:\n",
    "    df = pd.read_csv(f)\n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df_pc = df[df['dataset'] == 'partly_cloudy']\n",
    "df_pc_grouped = df_pc.groupby(['model','scattering_type', 'largest_scale', 'layer_list', 'moment_list', 'wavelet_type'])['score']\n",
    "mean_scores = df_pc_grouped.mean().reset_index()\n",
    "std_deviation = df_pc_grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                            \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scattering_type model            score\n",
      "0            blis   KNN   $97.5 \\pm 1.0$\n",
      "1            blis   MLP   $99.5 \\pm 0.4$\n",
      "2            blis    RF   $98.1 \\pm 0.7$\n",
      "3            blis   SVC  $100.0 \\pm 0.0$\n",
      "4            blis   XGB   $97.7 \\pm 0.7$\n",
      "5         modulus   KNN   $45.7 \\pm 5.4$\n",
      "6         modulus   MLP   $55.2 \\pm 6.7$\n",
      "7         modulus    RF   $40.2 \\pm 8.3$\n",
      "8         modulus   SVC   $56.2 \\pm 3.4$\n",
      "9         modulus   XGB   $45.9 \\pm 7.3$\n"
     ]
    }
   ],
   "source": [
    "grouped = gaussian_dataset.groupby(['scattering_type', 'model'])['score']\n",
    "\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scattering_type model           score\n",
      "0            blis   KNN  $93.4 \\pm 1.6$\n",
      "1            blis   MLP  $98.9 \\pm 0.5$\n",
      "2            blis    RF  $97.5 \\pm 0.6$\n",
      "3            blis   SVC  $97.7 \\pm 0.1$\n",
      "4            blis   XGB  $97.3 \\pm 0.1$\n",
      "5         modulus   KNN  $93.4 \\pm 1.6$\n",
      "6         modulus   MLP  $97.4 \\pm 0.1$\n",
      "7         modulus    RF  $96.3 \\pm 0.8$\n",
      "8         modulus   SVC  $97.4 \\pm 0.1$\n",
      "9         modulus   XGB  $95.4 \\pm 0.9$\n"
     ]
    }
   ],
   "source": [
    "# process the camel dataset\n",
    "\n",
    "camel_dataset = df_s1[df_s1['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "grouped = camel_dataset.groupby(['scattering_type', 'model'])['score']\n",
    "\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['score'] = \"$\" + (formatted_results['score'] * 100).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (std_deviation['score'] * 100).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sub_dataset', 'model', 'acc', 'stdev', 'hidden_dim', 'learning_rate'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sgnn = pd.read_csv('synthetic_GNN.csv')\n",
    "df_sgnn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model              acc\n",
      "0   GAT  $100.0 \\pm 0.0$\n",
      "1   GCN  $100.0 \\pm 0.0$\n",
      "2   GIN  $100.0 \\pm 0.1$\n"
     ]
    }
   ],
   "source": [
    "gaussian_dataset = df_sgnn[df_sgnn['sub_dataset'].str.startswith(\"gaussian_pm\")]\n",
    "grouped = gaussian_dataset.groupby(['model'])['acc']\n",
    "\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model             acc\n",
      "0   GAT  $94.6 \\pm 0.4$\n",
      "1   GCN  $94.4 \\pm 0.3$\n",
      "2   GIN  $88.9 \\pm 1.5$\n"
     ]
    }
   ],
   "source": [
    "camel_dataset = df_sgnn[df_sgnn['sub_dataset'].str.startswith(\"camel_pm\")]\n",
    "grouped = camel_dataset.groupby(['model'])['acc']\n",
    "\n",
    "mean_scores = grouped.mean().reset_index()\n",
    "std_deviation = grouped.std().reset_index()\n",
    "\n",
    "# Convert to percentages (without the percent sign) and format\n",
    "formatted_results = mean_scores.copy()\n",
    "formatted_results['acc'] = \"$\" + (formatted_results['acc'] ).map(\"{:.1f}\".format) + \\\n",
    "                             \" \\pm \" + (std_deviation['acc'] ).map(\"{:.1f}\".format) + \"$\"\n",
    "\n",
    "print(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'MLP'\n",
    "# Filter the DataFrame for only rows where the model is 'MLP'\n",
    "model_df = df[df['model'] == model]\n",
    "\n",
    "# Compute the average score for 'blis' scattering type\n",
    "blis_avg_score = model_df[model_df['scattering_type'] == 'blis']['score'].mean()\n",
    "print(f\"Average score for 'blis' scattering type with {model} model: {blis_avg_score:.4f}\")\n",
    "\n",
    "blis_avg_stdev = model_df[model_df['scattering_type'] == 'blis']['stdev'].mean()\n",
    "print(f\"Average standard deviation for 'blis' scattering type with {model} model: {blis_avg_stdev:.4f}\")\n",
    "\n",
    "# Compute the average score for 'modulus' scattering type\n",
    "modulus_avg_score = model_df[model_df['scattering_type'] == 'modulus']['score'].mean()\n",
    "print(f\"Average score for 'modulus' scattering type with {model} model: {modulus_avg_score:.4f}\")\n",
    "\n",
    "modulus_avg_stdev = model_df[model_df['scattering_type'] == 'modulus']['stdev'].mean()\n",
    "print(f\"Average standard deviation for 'modulus' scattering type with {model} model: {modulus_avg_stdev:.4f}\")\n",
    "\n",
    "print(f'Difference is {blis_avg_score - modulus_avg_score}')\n",
    "print(f'Fractional improvement is {(blis_avg_score - modulus_avg_score)/modulus_avg_score}')\n",
    "\n",
    "direct_improvements = []\n",
    "for sub_dataset in range(155):\n",
    "    blis_score = model_df[(model_df['scattering_type'] == 'blis') & (model_df['sub_dataset'] == sub_dataset)]['score']\n",
    "    modulus_score = model_df[(model_df['scattering_type'] == 'modulus') & (model_df['sub_dataset'] == sub_dataset)]['score']\n",
    "    diff = blis_score.item() - modulus_score.item()\n",
    "    direct_improvements.append(diff)\n",
    "\n",
    "direct_improvements = np.array(direct_improvements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (blis)",
   "language": "python",
   "name": "blis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
