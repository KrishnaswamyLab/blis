{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_scatter import scatter_add\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Author: Alex Tong\n",
    "## Reference: Data-Driven Learning of Geometric Scattering Networks, IEEE Machine Learning for Signal Processing Workshop 2021\n",
    "\n",
    "def scatter_moments(graph, batch_indices, moments_returned=4):\n",
    "    \n",
    "    \"\"\" Compute specified statistical coefficients for each feature of each graph passed. \n",
    "        The graphs expected are disjoint subgraphs within a single graph, whose feature tensor is passed as argument \"graph.\"\n",
    "        \"batch_indices\" connects each feature tensor to its home graph.\n",
    "        \"Moments_returned\" specifies the number of statistical measurements to compute. \n",
    "        If 1, only the mean is returned. If 2, the mean and variance. If 3, the mean, variance, and skew. If 4, the mean, variance, skew, and kurtosis.\n",
    "        The output is a dictionary. You can obtain the mean by calling output[\"mean\"] or output[\"skew\"], etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Aggregate the features of each mini-batch graph into its own tensor\n",
    "    graph_features = [torch.zeros(0).to(graph) for i in range(torch.max(batch_indices) + 1)]\n",
    "\n",
    "    for i, node_features in enumerate(graph):\n",
    "\n",
    "        # Sort the graph features by graph, according to batch_indices. For each graph, create a tensor whose first row is the first element of each feature, etc.\n",
    "        # print(\"node features are\", node_features)\n",
    "        \n",
    "        if (len(graph_features[batch_indices[i]]) == 0):  \n",
    "            # If this is the first feature added to this graph, fill it in with the features.\n",
    "            graph_features[batch_indices[i]] = node_features.view(-1, 1, 1)  # .view(-1,1,1) changes [1,2,3] to [[1],[2],[3]], so that we can add each column to the respective row.\n",
    "        else:\n",
    "            graph_features[batch_indices[i]] = torch.cat((graph_features[batch_indices[i]], node_features.view(-1, 1, 1)), dim=1)  # concatenates along columns\n",
    "\n",
    "    statistical_moments = {\"mean\": torch.zeros(0).to(graph)}\n",
    "\n",
    "    if moments_returned >= 2:\n",
    "        statistical_moments[\"variance\"] = torch.zeros(0).to(graph)\n",
    "    if moments_returned >= 3:\n",
    "        statistical_moments[\"skew\"] = torch.zeros(0).to(graph)\n",
    "    if moments_returned >= 4:\n",
    "        statistical_moments[\"kurtosis\"] = torch.zeros(0).to(graph)\n",
    "\n",
    "    for data in graph_features:\n",
    "\n",
    "        data = data.squeeze()\n",
    "        \n",
    "        def m(i):  # ith moment, computed with derivation data\n",
    "            return torch.mean(deviation_data ** i, axis=1)\n",
    "\n",
    "        mean = torch.mean(data, dim=1, keepdim=True)\n",
    "        \n",
    "        if moments_returned >= 1:\n",
    "            statistical_moments[\"mean\"] = torch.cat(\n",
    "                (statistical_moments[\"mean\"], mean.T), dim=0\n",
    "            )\n",
    "\n",
    "        # produce matrix whose every row is data row - mean of data row\n",
    "\n",
    "        #for a in mean:\n",
    "        #    mean_row = torch.ones(data.shape[1]).to( * a\n",
    "        #    tuple_collect.append(\n",
    "        #        mean_row[None, ...]\n",
    "        #    )  # added dimension to concatenate with differentiation of rows\n",
    "        # each row contains the deviation of the elements from the mean of the row\n",
    "        \n",
    "        deviation_data = data - mean\n",
    "        \n",
    "        # variance: difference of u and u mean, squared element wise, summed and divided by n-1\n",
    "        variance = m(2)\n",
    "        \n",
    "        if moments_returned >= 2:\n",
    "            statistical_moments[\"variance\"] = torch.cat(\n",
    "                (statistical_moments[\"variance\"], variance[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "        # skew: 3rd moment divided by cubed standard deviation (sd = sqrt variance), with correction for division by zero (inf -> 0)\n",
    "        skew = m(3) / (variance ** (3 / 2)) \n",
    "        skew[\n",
    "            skew > 1000000000000000\n",
    "        ] = 0  # multivalued tensor division by zero produces inf\n",
    "        skew[\n",
    "            skew != skew\n",
    "        ] = 0  # single valued division by 0 produces nan. In both cases we replace with 0.\n",
    "        if moments_returned >= 3:\n",
    "            statistical_moments[\"skew\"] = torch.cat(\n",
    "                (statistical_moments[\"skew\"], skew[None, ...]), dim=0\n",
    "            )\n",
    "\n",
    "        # kurtosis: fourth moment, divided by variance squared. Using Fischer's definition to subtract 3 (default in scipy)\n",
    "        kurtosis = m(4) / (variance ** 2) - 3 \n",
    "        kurtosis[kurtosis > 1000000000000000] = -3\n",
    "        kurtosis[kurtosis != kurtosis] = -3\n",
    "        if moments_returned >= 4:\n",
    "            statistical_moments[\"kurtosis\"] = torch.cat(\n",
    "                (statistical_moments[\"kurtosis\"], kurtosis[None, ...]), dim=0\n",
    "            )\n",
    "    \n",
    "    # Concatenate into one tensor (alex)\n",
    "    statistical_moments = torch.cat([v for k,v in statistical_moments.items()], axis=1)\n",
    "    #statistical_moments = torch.cat([statistical_moments['mean'],statistical_moments['variance']],axis=1)\n",
    "    \n",
    "    return statistical_moments\n",
    "\n",
    "\n",
    "class LazyLayer(torch.nn.Module):\n",
    "    \n",
    "    \"\"\" Currently a single elementwise multiplication with one laziness parameter per\n",
    "    channel. this is run through a softmax so that this is a real laziness parameter\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.weights = torch.nn.Parameter(torch.Tensor(2, n))\n",
    "\n",
    "    def forward(self, x, propogated):\n",
    "        inp = torch.stack((x, propogated), dim=1)\n",
    "        s_weights = torch.nn.functional.softmax(self.weights, dim=0)\n",
    "        return torch.sum(inp * s_weights, dim=-2)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.ones_(self.weights)\n",
    "    \n",
    "\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, add_self_loops=False, dtype=None):\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, tmp_edge_weight = add_remaining_self_loops(\n",
    "            edge_index, edge_weight, 1, num_nodes)\n",
    "        assert tmp_edge_weight is not None\n",
    "        edge_weight = tmp_edge_weight\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow_(-1)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    \n",
    "    return edge_index, deg_inv_sqrt[col] * edge_weight\n",
    "\n",
    "\n",
    "class Diffuse(MessagePassing):\n",
    "\n",
    "    \"\"\" Implements low pass walk with optional weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, trainable_laziness=False, fixed_weights=True):\n",
    "\n",
    "        super().__init__(aggr=\"add\",  flow = \"target_to_source\", node_dim=-3)  # \"Add\" aggregation.\n",
    "        assert in_channels == out_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.fixed_weights = fixed_weights\n",
    "        if trainable_laziness:\n",
    "            self.lazy_layer = LazyLayer(in_channels)\n",
    "        if not self.fixed_weights:\n",
    "            self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        # turn off this step for simplicity\n",
    "        if not self.fixed_weights:\n",
    "            x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization\n",
    "        edge_index, edge_weight = gcn_norm(edge_index, edge_weight, x.size(self.node_dim), dtype=x.dtype)\n",
    "\n",
    "        # Step 4-6: Start propagating messages.\n",
    "        propogated = self.propagate(\n",
    "            edge_index, edge_weight=edge_weight, size=None, x=x,\n",
    "        )\n",
    "        if not self.trainable_laziness:\n",
    "            return 0.5 * (x + propogated), edge_index, edge_weight\n",
    "\n",
    "        return self.lazy_layer(x, propogated), edge_index, edge_weight\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        \n",
    "        # x_j has shape [E, out_channels]\n",
    "        # Step 4: Normalize node features.\n",
    "        return edge_weight.view(-1, 1, 1) * x_j\n",
    "\n",
    "\n",
    "    #def message_and_aggregate(self, adj_t, x):\n",
    "    #\n",
    "    #    return matmul(adj_t, x, reduce=self.aggr)\n",
    "\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "\n",
    "        # aggr_out has shape [N, out_channels]\n",
    "        # Step 6: Return new node embeddings.\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "def feng_filters():\n",
    "\n",
    "    tmp = np.arange(16).reshape(4,4) #tmp doesn't seem to be used!\n",
    "    results = [4]\n",
    "    for i in range(2, 4):\n",
    "        for j in range(0, i):\n",
    "            results.append(4*i+j)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class Scatter(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, trainable_laziness=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.diffusion_layer1 = Diffuse(in_channels, in_channels, trainable_laziness)\n",
    "        self.diffusion_layer2 = Diffuse(\n",
    "            4 * in_channels, 4 * in_channels, trainable_laziness\n",
    "        )\n",
    "        self.wavelet_constructor = torch.nn.Parameter(torch.tensor([\n",
    "            [0, -1.0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        ], requires_grad=True))\n",
    "        # self.wavelet_constructor = torch.tensor([\n",
    "        #     [0, -1.0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        #     [0, 0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        #     [0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        #     [0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        # ], device=device)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        s0 = x[:,:,None]\n",
    "        avgs = [s0]\n",
    "        for i in range(16):\n",
    "            avgs.append(self.diffusion_layer1(avgs[-1], edge_index)[0])\n",
    "        for j in range(len(avgs)):\n",
    "            avgs[j] = avgs[j][None, :, :, :]  # add an extra dimension to each tensor to avoid data loss while concatenating TODO: is there a faster way to do this?\n",
    "        \n",
    "        # Combine the diffusion levels into a single tensor.\n",
    "        diffusion_levels = torch.cat(avgs)\n",
    "        \n",
    "        # Reshape the 3d tensor into a 2d tensor and multiply with the wavelet_constructor matrix\n",
    "        # This simulates the below subtraction:\n",
    "        # filter1 = avgs[1] - avgs[2]\n",
    "        # filter2 = avgs[2] - avgs[4]\n",
    "        # filter3 = avgs[4] - avgs[8]\n",
    "        # filter4 = avgs[8] - avgs[16]\n",
    "        subtracted = torch.matmul(self.wavelet_constructor, diffusion_levels.view(17, -1))\n",
    "        subtracted = subtracted.view(4, x.shape[0], x.shape[1]) # reshape into given input shape\n",
    "        s1 = torch.abs(\n",
    "            torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "\n",
    "        # perform a second wave of diffusing, on the recently diffused.\n",
    "        avgs = [s1]\n",
    "        for i in range(16): # diffuse over diffusions\n",
    "            avgs.append(self.diffusion_layer2(avgs[-1], edge_index))\n",
    "        for i in range(len(avgs)): # add an extra dimension to each diffusion level for concatenation\n",
    "            avgs[i] = avgs[i][None, :, :, :]\n",
    "        diffusion_levels2 = torch.cat(avgs)\n",
    "        \n",
    "        # Having now generated the diffusion levels, we can cmobine them as before\n",
    "        subtracted2 = torch.matmul(self.wavelet_constructor, diffusion_levels2.view(17, -1))\n",
    "        subtracted2 = subtracted2.view(4, s1.shape[0], s1.shape[1], s1.shape[2])  # reshape into given input shape\n",
    "        subtracted2 = torch.transpose(subtracted2, 0, 1)\n",
    "        subtracted2 = torch.abs(subtracted2.reshape(-1, self.in_channels, 4))\n",
    "        s2_swapped = torch.reshape(torch.transpose(subtracted2, 1, 2), (-1, 16, self.in_channels))\n",
    "        s2 = s2_swapped[:, feng_filters()]\n",
    "\n",
    "        x = torch.cat([s0, s1], dim=2)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = torch.cat([x, s2], dim=1)\n",
    "\n",
    "        #x = scatter_mean(x, batch, dim=0)\n",
    "        if hasattr(data, 'batch'):\n",
    "            x = scatter_moments(x, data.batch, 4)\n",
    "        else:\n",
    "            x = scatter_moments(x, torch.zeros(data.x.shape[0], dtype=torch.int32), 4)\n",
    "            # print('x returned shape', x.shape)\n",
    "        return x, self.wavelet_constructor\n",
    "\n",
    "\n",
    "    def out_shape(self):\n",
    "\n",
    "        # x * 4 moments * in\n",
    "        return 11 * 4 * self.in_channels\n",
    "\n",
    "\n",
    "class TSNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, edge_in_channels = None, trainable_laziness=False, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_in_channels = edge_in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.scatter = Scatter(in_channels, trainable_laziness=trainable_laziness)\n",
    "        self.lin1 = Linear(self.scatter.out_shape(), out_channels)\n",
    "        self.lin2 = Linear(out_channels, out_channels)\n",
    "        self.lin3 = Linear(out_channels, out_channels)\n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, sc = self.scatter(data)\n",
    "        x = self.act(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)\n",
    "        return x, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blis layer and Blis net initial definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Blis(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, trainable_laziness=False, activation = \"blis\"):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.diffusion_layer1 = Diffuse(in_channels, in_channels, trainable_laziness)\n",
    "        # self.diffusion_layer2 = Diffuse(\n",
    "        #     4 * in_channels, 4 * in_channels, trainable_laziness\n",
    "        # )\n",
    "        self.wavelet_constructor = torch.nn.Parameter(torch.tensor([\n",
    "            [1, -1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, -1],\n",
    "            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "        ], requires_grad=True))\n",
    "\n",
    "        if activation == \"blis\":\n",
    "            self.activations = [lambda x: torch.relu(x), lambda x: torch.relu(-x)]\n",
    "        elif activation == None:\n",
    "            self.activations = [lambda x : x]\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        s0 = x[:,:,None]\n",
    "        avgs = [s0]\n",
    "        for i in range(16):\n",
    "            avgs.append(self.diffusion_layer1(avgs[-1], edge_index)[0])\n",
    "        for j in range(len(avgs)):\n",
    "            avgs[j] = avgs[j][None, :, :, :]  # add an extra dimension to each tensor to avoid data loss while concatenating TODO: is there a faster way to do this?\n",
    "        \n",
    "        # Combine the diffusion levels into a single tensor.\n",
    "        diffusion_levels = torch.cat(avgs)\n",
    "        \n",
    "        # Reshape the 3d tensor into a 2d tensor and multiply with the wavelet_constructor matrix\n",
    "        # This simulates the below subtraction:\n",
    "        # filter0 = avgs[0] - avgs[1]\n",
    "        # filter1 = avgs[1] - avgs[2] \n",
    "        # filter2 = avgs[2] - avgs[4]\n",
    "        # filter3 = avgs[4] - avgs[8]\n",
    "        # filter4 = avgs[8] - avgs[16] \n",
    "        # filter5 = avgs[16]\n",
    "        #subtracted = torch.matmul(self.wavelet_constructor, diffusion_levels.view(17, -1))\n",
    "        wavelet_coeffs = torch.einsum(\"ij,jklm->iklm\", self.wavelet_constructor, diffusion_levels) # J x num_nodes x num_features x 1\n",
    "        #subtracted = subtracted.view(6, x.shape[0], x.shape[1]) # reshape into given input shape\n",
    "        activated = [self.activations[i](wavelet_coeffs) for i in range(len(self.activations))]\n",
    "        #s1 = self.relu(\n",
    "        #    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "        #s2 = self.relu(-\n",
    "        #    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "        s = torch.cat(activated, axis=-1).transpose(1,0)\n",
    "        #x = torch.cat((s1,s2), axis = -1)\n",
    "        #print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        return s\n",
    "        return s.reshape(x.shape[0],-1), diffusion_levels, self.wavelet_constructor\n",
    "\n",
    "    def out_features(self):\n",
    "\n",
    "        # x * 4 moments * in\n",
    "        return 12 * self.in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edebrouwer/YaleLocal/bliss/blis/models/wavelets.py:7: RuntimeWarning: divide by zero encountered in divide\n",
      "  d_arr_inv = 1/d_arr\n"
     ]
    }
   ],
   "source": [
    "from blis import DATA_DIR\n",
    "import os\n",
    "import blis.models.scattering_transform as st \n",
    "import blis.models.wavelets as wav \n",
    "\n",
    "dataset = 'traffic'\n",
    "sub_dataset = \"PEMS04\"\n",
    "label= \"DAY\"\n",
    "largest_scale = 4\n",
    "scattering_type = 'blis'\n",
    "num_layers = 1\n",
    "highest_moment = 4\n",
    "wavelet_type = 'W2'\n",
    "\n",
    "dataset_dir = os.path.join(DATA_DIR, dataset, sub_dataset)\n",
    "processed_dir =  os.path.join(dataset_dir, 'processed', scattering_type, wavelet_type, f'largest_scale_{largest_scale}')\n",
    "\n",
    "\n",
    "dataset_dir = os.path.join(DATA_DIR, dataset, sub_dataset)\n",
    "\n",
    "\n",
    "# load adjacency matrix and signal\n",
    "A = np.load(os.path.join(dataset_dir, 'adjacency_matrix.npy'), allow_pickle = True)\n",
    "x = np.load(os.path.join(dataset_dir, 'graph_signals.npy'), allow_pickle = True)\n",
    "y = np.load(os.path.join(dataset_dir, label, 'label.npy'), allow_pickle = True)\n",
    "if len(x.shape) == 2:\n",
    "    x = x[:,:,None]\n",
    "\n",
    "x = x[:10]\n",
    "\n",
    "if wavelet_type == 'W2':\n",
    "    wavelets = wav.get_W_2(A, largest_scale, low_pass_as_wavelet=(scattering_type == 'blis'))\n",
    "else:\n",
    "    wavelets = wav.get_W_1(A, largest_scale, low_pass_as_wavelet=(scattering_type == 'blis'))\n",
    "#coeffs = st.scattering_transform(x, scattering_type, wavelets, num_layers, highest_moment, None)\n",
    "\n",
    "\n",
    "coeffs = np.stack([np.einsum('ik, nkf->nif', wavelets[j], x) for j in range(len(wavelets))],1).transpose(0,2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 307, 6, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 307, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs[0].transpose(1,0,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from blis.data.load_from_np import create_dataset\n",
    "\n",
    "data_list = create_dataset(x, y , A)\n",
    "\n",
    "blis_mod = Blis(in_channels = 3, trainable_laziness=False, activation = None)\n",
    "out_coeffs  = blis_mod(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([307, 6, 3, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.070570521439663e-05"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(coeffs[0] - out_coeffs.detach().numpy()[...,0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285.2721862792969"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(coeffs[0].transpose(1,0,2).reshape(len(out_coeffs),-1) - out_coeffs.detach().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtracted = subtracted.view(6, x.shape[0], x.shape[1]) # reshape into given input shape\n",
    "        #s1 = self.relu(\n",
    "        #    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "        #s2 = self.relu(-\n",
    "        #    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "        #x = torch.cat((s1,s2), axis = -1)\n",
    "        #print(x.shape)\n",
    "        #x = x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 307, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_coeffs.view(6, 307,3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 921])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_coeffs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted = out_coeffs\n",
    "subtracted = subtracted.view(6, x.shape[0], x.shape[1]) # reshape into given input shape\n",
    "s1 = self.relu(\n",
    "    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "s2 = self.relu(-\n",
    "    torch.transpose(torch.transpose(subtracted, 0, 1), 1, 2))  # transpose the dimensions to match previous\n",
    "x = torch.cat((s1,s2), axis = -1)\n",
    "print(x.shape)\n",
    "x = x.reshape(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.070570521439663e-05"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(coeffs[0] - out_coeffs.detach().numpy()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index = data_list[0].x, data_list[0].edge_index\n",
    "s0 = x[:,:,None]\n",
    "avgs = [s0]\n",
    "diffusion_layer1 = Diffuse( 1, 1, trainable_laziness = False, fixed_weights= True)\n",
    "diff_out, edge_index_, edge_weights_ = diffusion_layer1(avgs[-1], edge_index)\n",
    "\n",
    "A_l = torch_geometric.utils.to_dense_adj(edge_index_, edge_attr = edge_weights_).numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mt/chc1ffgd6vbcj65ckwqbppw80000gn/T/ipykernel_55295/3755934023.py:4: RuntimeWarning: divide by zero encountered in divide\n",
      "  d_arr_inv = 1/d_arr\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric\n",
    "A_ = torch_geometric.utils.to_dense_adj(edge_index).numpy()[0]\n",
    "d_arr = np.sum(A_, axis=0)\n",
    "d_arr_inv = 1/d_arr\n",
    "d_arr_inv[np.isinf(d_arr_inv)] = 0\n",
    "D_inv = np.diag(d_arr_inv)\n",
    "P_ =  A_ @ D_inv\n",
    "\n",
    "x_ = (P_ @ x.numpy())\n",
    "x_ = 0.5 * ( x_ + x.numpy() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6293945e-06"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(x_ - diff_out.detach().numpy()[...,0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edebrouwer/YaleLocal/bliss/blis/models/wavelets.py:7: RuntimeWarning: divide by zero encountered in divide\n",
      "  d_arr_inv = 1/d_arr\n"
     ]
    }
   ],
   "source": [
    "A_ = torch_geometric.utils.to_dense_adj(edge_index).numpy()[0]\n",
    "from blis.models.wavelets import get_P\n",
    "P_b = get_P(A_)\n",
    "x_ = (P_b @ x.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(0.5 * (np.identity(len(P_)) + P_) - P_b).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.62939453125e-06"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(x_ - diff_out.detach().numpy()[...,0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.norm import BatchNorm\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "from torch_geometric.nn import GCNConv\n",
    "class BlisNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, edge_in_channels = None, trainable_laziness=False, **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.edge_in_channels = edge_in_channels\n",
    "        self.trainable_laziness = trainable_laziness\n",
    "        self.conv1 = GCNConv(in_channels, in_channels)\n",
    "        self.blis1 = Blis(in_channels, trainable_laziness=trainable_laziness)\n",
    "        self.blis2 = Blis(self.blis1.out_features(), trainable_laziness=trainable_laziness)\n",
    "        self.batch_norm = BatchNorm(self.blis2.out_features())\n",
    "        self.lin1 = Linear(self.blis2.out_features(), self.blis2.out_features()//2 )\n",
    "        self.mean = global_mean_pool\n",
    "        self.lin2 = Linear(self.blis2.out_features()//2, out_channels)\n",
    "        self.lin3 = Linear(out_channels, out_channels)\n",
    "\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 1. Obtain node embeddings \n",
    "        data.x = self.conv1(data.x, data.edge_index)\n",
    "        #x = x.relu()\n",
    "        x, sc1 = self.blis1(data)\n",
    "        data.x = x # there's gotta be a better way to do this...\n",
    "        x, sc2 = self.blis2(data)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.mean(x,data.batch)\n",
    "        x = self.lin2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUDataset and synthetic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2454], x=[1118, 7], edge_attr=[2454, 4], y=[64], batch=[1118], ptr=[65])\n",
      "torch.Size([1118, 7, 12])\n",
      "DataBatch(edge_index=[2, 2712], x=[1220, 7], edge_attr=[2712, 4], y=[64], batch=[1220], ptr=[65])\n",
      "torch.Size([1220, 7, 12])\n",
      "DataBatch(edge_index=[2, 828], x=[376, 7], edge_attr=[828, 4], y=[22], batch=[376], ptr=[23])\n",
      "torch.Size([376, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "model = Blis(dataset.num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = .01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)\n",
    "    out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(34323)\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.7133, Test Acc: 0.7632\n",
      "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 006, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 007, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
      "Epoch: 009, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 010, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 011, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 012, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 013, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 014, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 015, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 016, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7368\n",
      "Epoch: 018, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 019, Train Acc: 0.7667, Test Acc: 0.7368\n",
      "Epoch: 020, Train Acc: 0.8200, Test Acc: 0.6579\n",
      "Epoch: 021, Train Acc: 0.8267, Test Acc: 0.7632\n",
      "Epoch: 022, Train Acc: 0.8400, Test Acc: 0.6579\n",
      "Epoch: 023, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 024, Train Acc: 0.8067, Test Acc: 0.8158\n",
      "Epoch: 025, Train Acc: 0.8533, Test Acc: 0.8158\n",
      "Epoch: 026, Train Acc: 0.8600, Test Acc: 0.8158\n",
      "Epoch: 027, Train Acc: 0.5933, Test Acc: 0.4737\n",
      "Epoch: 028, Train Acc: 0.3667, Test Acc: 0.2368\n",
      "Epoch: 029, Train Acc: 0.4267, Test Acc: 0.2632\n",
      "Epoch: 030, Train Acc: 0.8667, Test Acc: 0.7632\n",
      "Epoch: 031, Train Acc: 0.7867, Test Acc: 0.6316\n",
      "Epoch: 032, Train Acc: 0.3600, Test Acc: 0.2105\n",
      "Epoch: 033, Train Acc: 0.3600, Test Acc: 0.2368\n",
      "Epoch: 034, Train Acc: 0.4733, Test Acc: 0.3158\n",
      "Epoch: 035, Train Acc: 0.8667, Test Acc: 0.6842\n",
      "Epoch: 036, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 037, Train Acc: 0.8000, Test Acc: 0.7895\n",
      "Epoch: 038, Train Acc: 0.7533, Test Acc: 0.7895\n",
      "Epoch: 039, Train Acc: 0.8133, Test Acc: 0.7895\n",
      "Epoch: 040, Train Acc: 0.8333, Test Acc: 0.7895\n",
      "Epoch: 041, Train Acc: 0.3600, Test Acc: 0.2105\n",
      "Epoch: 042, Train Acc: 0.3600, Test Acc: 0.2105\n",
      "Epoch: 043, Train Acc: 0.6467, Test Acc: 0.4737\n",
      "Epoch: 044, Train Acc: 0.8933, Test Acc: 0.7368\n",
      "Epoch: 045, Train Acc: 0.8867, Test Acc: 0.7632\n",
      "Epoch: 046, Train Acc: 0.8333, Test Acc: 0.7895\n",
      "Epoch: 047, Train Acc: 0.8000, Test Acc: 0.6842\n",
      "Epoch: 048, Train Acc: 0.7267, Test Acc: 0.6053\n",
      "Epoch: 049, Train Acc: 0.8467, Test Acc: 0.7105\n",
      "Epoch: 050, Train Acc: 0.9333, Test Acc: 0.7632\n",
      "Epoch: 051, Train Acc: 0.8800, Test Acc: 0.7895\n",
      "Epoch: 052, Train Acc: 0.9000, Test Acc: 0.7895\n",
      "Epoch: 053, Train Acc: 0.8733, Test Acc: 0.8158\n",
      "Epoch: 054, Train Acc: 0.6400, Test Acc: 0.4211\n",
      "Epoch: 055, Train Acc: 0.7667, Test Acc: 0.6316\n",
      "Epoch: 056, Train Acc: 0.6867, Test Acc: 0.5526\n",
      "Epoch: 057, Train Acc: 0.8800, Test Acc: 0.6579\n",
      "Epoch: 058, Train Acc: 0.9200, Test Acc: 0.7895\n",
      "Epoch: 059, Train Acc: 0.9267, Test Acc: 0.7895\n",
      "Epoch: 060, Train Acc: 0.9400, Test Acc: 0.7632\n",
      "Epoch: 061, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 062, Train Acc: 0.9533, Test Acc: 0.7895\n",
      "Epoch: 063, Train Acc: 0.7267, Test Acc: 0.7895\n",
      "Epoch: 064, Train Acc: 0.8067, Test Acc: 0.7632\n",
      "Epoch: 065, Train Acc: 0.8000, Test Acc: 0.7895\n",
      "Epoch: 066, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 067, Train Acc: 0.8400, Test Acc: 0.7368\n",
      "Epoch: 068, Train Acc: 0.8933, Test Acc: 0.7105\n",
      "Epoch: 069, Train Acc: 0.8733, Test Acc: 0.7368\n",
      "Epoch: 070, Train Acc: 0.8333, Test Acc: 0.7895\n",
      "Epoch: 071, Train Acc: 0.9067, Test Acc: 0.7105\n",
      "Epoch: 072, Train Acc: 0.9400, Test Acc: 0.7368\n",
      "Epoch: 073, Train Acc: 0.9333, Test Acc: 0.7368\n",
      "Epoch: 074, Train Acc: 0.9467, Test Acc: 0.7368\n",
      "Epoch: 075, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 076, Train Acc: 0.9200, Test Acc: 0.7368\n",
      "Epoch: 077, Train Acc: 0.9533, Test Acc: 0.7105\n",
      "Epoch: 078, Train Acc: 0.9267, Test Acc: 0.7368\n",
      "Epoch: 079, Train Acc: 0.9267, Test Acc: 0.7632\n",
      "Epoch: 080, Train Acc: 0.9467, Test Acc: 0.7105\n",
      "Epoch: 081, Train Acc: 0.9267, Test Acc: 0.7632\n",
      "Epoch: 082, Train Acc: 0.9133, Test Acc: 0.7368\n",
      "Epoch: 083, Train Acc: 0.9533, Test Acc: 0.7105\n",
      "Epoch: 084, Train Acc: 0.9400, Test Acc: 0.7632\n",
      "Epoch: 085, Train Acc: 0.9600, Test Acc: 0.7632\n",
      "Epoch: 086, Train Acc: 0.8000, Test Acc: 0.5789\n",
      "Epoch: 087, Train Acc: 0.8867, Test Acc: 0.7895\n",
      "Epoch: 088, Train Acc: 0.8933, Test Acc: 0.7632\n",
      "Epoch: 089, Train Acc: 0.9267, Test Acc: 0.7368\n",
      "Epoch: 090, Train Acc: 0.9333, Test Acc: 0.7368\n",
      "Epoch: 091, Train Acc: 0.9400, Test Acc: 0.7632\n",
      "Epoch: 092, Train Acc: 0.9600, Test Acc: 0.7632\n",
      "Epoch: 093, Train Acc: 0.9533, Test Acc: 0.7895\n",
      "Epoch: 094, Train Acc: 0.3600, Test Acc: 0.2368\n",
      "Epoch: 095, Train Acc: 0.3667, Test Acc: 0.2105\n",
      "Epoch: 096, Train Acc: 0.9467, Test Acc: 0.7105\n",
      "Epoch: 097, Train Acc: 0.8667, Test Acc: 0.7368\n",
      "Epoch: 098, Train Acc: 0.7467, Test Acc: 0.7632\n",
      "Epoch: 099, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 100, Train Acc: 0.6733, Test Acc: 0.3684\n",
      "Epoch: 101, Train Acc: 0.8067, Test Acc: 0.5789\n",
      "Epoch: 102, Train Acc: 0.9600, Test Acc: 0.7105\n",
      "Epoch: 103, Train Acc: 0.9400, Test Acc: 0.6842\n",
      "Epoch: 104, Train Acc: 0.9133, Test Acc: 0.6842\n",
      "Epoch: 105, Train Acc: 0.9200, Test Acc: 0.7632\n",
      "Epoch: 106, Train Acc: 0.9133, Test Acc: 0.7105\n",
      "Epoch: 107, Train Acc: 0.9467, Test Acc: 0.7632\n",
      "Epoch: 108, Train Acc: 0.8933, Test Acc: 0.7105\n",
      "Epoch: 109, Train Acc: 0.8933, Test Acc: 0.6053\n",
      "Epoch: 110, Train Acc: 0.9267, Test Acc: 0.6842\n",
      "Epoch: 111, Train Acc: 0.9067, Test Acc: 0.7632\n",
      "Epoch: 112, Train Acc: 0.9133, Test Acc: 0.7632\n",
      "Epoch: 113, Train Acc: 0.8733, Test Acc: 0.7895\n",
      "Epoch: 114, Train Acc: 0.9200, Test Acc: 0.8158\n",
      "Epoch: 115, Train Acc: 0.7933, Test Acc: 0.7632\n",
      "Epoch: 116, Train Acc: 0.8667, Test Acc: 0.7895\n",
      "Epoch: 117, Train Acc: 0.8733, Test Acc: 0.7895\n",
      "Epoch: 118, Train Acc: 0.8333, Test Acc: 0.7895\n",
      "Epoch: 119, Train Acc: 0.9267, Test Acc: 0.7632\n",
      "Epoch: 120, Train Acc: 0.9533, Test Acc: 0.8158\n",
      "Epoch: 121, Train Acc: 0.9600, Test Acc: 0.7895\n",
      "Epoch: 122, Train Acc: 0.9533, Test Acc: 0.7632\n",
      "Epoch: 123, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 124, Train Acc: 0.9400, Test Acc: 0.7895\n",
      "Epoch: 125, Train Acc: 0.8333, Test Acc: 0.8158\n",
      "Epoch: 126, Train Acc: 0.9067, Test Acc: 0.8158\n",
      "Epoch: 127, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 128, Train Acc: 0.8867, Test Acc: 0.8158\n",
      "Epoch: 129, Train Acc: 0.9467, Test Acc: 0.7895\n",
      "Epoch: 130, Train Acc: 0.9467, Test Acc: 0.8158\n",
      "Epoch: 131, Train Acc: 0.9200, Test Acc: 0.8158\n",
      "Epoch: 132, Train Acc: 0.9333, Test Acc: 0.8421\n",
      "Epoch: 133, Train Acc: 0.9733, Test Acc: 0.8158\n",
      "Epoch: 134, Train Acc: 0.9600, Test Acc: 0.7895\n",
      "Epoch: 135, Train Acc: 0.9267, Test Acc: 0.8158\n",
      "Epoch: 136, Train Acc: 0.9400, Test Acc: 0.7895\n",
      "Epoch: 137, Train Acc: 0.9800, Test Acc: 0.8158\n",
      "Epoch: 138, Train Acc: 0.9400, Test Acc: 0.8158\n",
      "Epoch: 139, Train Acc: 0.9467, Test Acc: 0.8158\n",
      "Epoch: 140, Train Acc: 0.9733, Test Acc: 0.8158\n",
      "Epoch: 141, Train Acc: 0.9733, Test Acc: 0.8158\n",
      "Epoch: 142, Train Acc: 0.7933, Test Acc: 0.5000\n",
      "Epoch: 143, Train Acc: 0.7467, Test Acc: 0.4474\n",
      "Epoch: 144, Train Acc: 0.8267, Test Acc: 0.5263\n",
      "Epoch: 145, Train Acc: 0.8333, Test Acc: 0.5526\n",
      "Epoch: 146, Train Acc: 0.8733, Test Acc: 0.6053\n",
      "Epoch: 147, Train Acc: 0.9667, Test Acc: 0.7895\n",
      "Epoch: 148, Train Acc: 0.9667, Test Acc: 0.7895\n",
      "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.6316\n",
      "Epoch: 150, Train Acc: 0.9400, Test Acc: 0.8684\n",
      "Epoch: 151, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 152, Train Acc: 0.9800, Test Acc: 0.7895\n",
      "Epoch: 153, Train Acc: 0.9467, Test Acc: 0.8158\n",
      "Epoch: 154, Train Acc: 0.9533, Test Acc: 0.7895\n",
      "Epoch: 155, Train Acc: 0.9733, Test Acc: 0.7632\n",
      "Epoch: 156, Train Acc: 0.9733, Test Acc: 0.7368\n",
      "Epoch: 157, Train Acc: 0.9533, Test Acc: 0.8421\n",
      "Epoch: 158, Train Acc: 0.9667, Test Acc: 0.8421\n",
      "Epoch: 159, Train Acc: 0.7667, Test Acc: 0.4474\n",
      "Epoch: 160, Train Acc: 0.8267, Test Acc: 0.7895\n",
      "Epoch: 161, Train Acc: 0.9067, Test Acc: 0.6316\n",
      "Epoch: 162, Train Acc: 0.9267, Test Acc: 0.7368\n",
      "Epoch: 163, Train Acc: 0.9333, Test Acc: 0.8158\n",
      "Epoch: 164, Train Acc: 0.9400, Test Acc: 0.8158\n",
      "Epoch: 165, Train Acc: 0.9600, Test Acc: 0.7895\n",
      "Epoch: 166, Train Acc: 0.9800, Test Acc: 0.8158\n",
      "Epoch: 167, Train Acc: 0.9667, Test Acc: 0.8158\n",
      "Epoch: 168, Train Acc: 0.9667, Test Acc: 0.8158\n",
      "Epoch: 169, Train Acc: 0.9733, Test Acc: 0.7895\n",
      "Epoch: 170, Train Acc: 0.9733, Test Acc: 0.7895\n"
     ]
    }
   ],
   "source": [
    "model = BlisNet(dataset.num_node_features, dataset.num_classes, trainable_laziness=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 12])\n",
      "[[9.13871765e+00 9.96826172e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.16651306e+01 0.00000000e+00 0.00000000e+00\n",
      "  2.38974380e+00 2.64044571e+00 6.14662170e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.87577362e+01 1.67672081e+01 8.36125755e+00\n",
      "  6.56978416e+00 2.51951599e+00 3.21403503e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 6.02922592e+01 7.72358322e+00 3.39667511e+00\n",
      "  2.38058472e+00 9.28695679e-01 1.79817200e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.87538071e+01 1.84674015e+01 8.42055702e+00\n",
      "  5.83637238e+00 1.95967865e+00 2.88284302e-01 0.00000000e+00]\n",
      " [1.07378693e+01 7.08545685e+00 7.19158173e+00 3.69013214e+00\n",
      "  6.28135681e-01 4.74000854e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.32472382e+01 1.23107834e+01 5.79267120e+00 2.25318909e-01\n",
      "  0.00000000e+00 4.30678329e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.92390442e-01 0.00000000e+00]\n",
      " [1.50113754e+01 5.80090714e+00 2.93370056e+00 3.31878662e-02\n",
      "  0.00000000e+00 3.87389069e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 2.95196533e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.01995850e-01 5.60186005e+01 2.82610741e+01 1.09768639e+01\n",
      "  6.07515335e+00 9.50389862e-01 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 5.82252502e-01\n",
      "  1.64848328e-01 4.73829918e+01 1.04374123e+01 3.30835724e+00\n",
      "  8.55716705e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.24128342e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.59759521e+01 0.00000000e+00 1.45471191e+00\n",
      "  4.91878510e+00 3.66986084e+00 6.86943054e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 6.88954010e+01 1.79326859e+01 1.00298538e+01\n",
      "  8.66354370e+00 3.69972229e+00 5.65765381e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 6.80915833e-01\n",
      "  9.20372009e-02 6.02932739e+01 1.07837715e+01 3.49983215e+00\n",
      "  8.07140350e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.96610794e+01 9.21894836e+00 6.64038086e+00 2.40717316e+00\n",
      "  3.95355225e-01 5.60109406e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  4.78057861e-02 5.16920090e+01 6.80240631e-01 5.13435364e-01\n",
      "  3.52912903e-01 9.71984863e-03 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.20661163e-01 4.74055748e+01 2.61705303e+01 1.12625809e+01\n",
      "  7.03569794e+00 1.32384491e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.58240604e+01 1.28921309e+01 5.37938213e+00\n",
      "  3.75278664e+00 1.64779472e+00 3.85082245e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 6.02694931e+01 1.27615604e+01 7.46218109e+00\n",
      "  7.32369995e+00 4.16547394e+00 8.89991760e-01 0.00000000e+00]\n",
      " [2.56282730e+01 1.38162956e+01 1.21333122e+01 5.65908051e+00\n",
      "  8.96568298e-01 3.87865219e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.32137680e+00 1.31725311e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 3.87378426e+01 0.00000000e+00 0.00000000e+00\n",
      "  9.82364655e-01 1.79041290e+00 5.45978546e-01 0.00000000e+00]\n",
      " [2.07445297e+01 1.26580582e+01 1.21566010e+01 6.01115036e+00\n",
      "  1.00726700e+00 4.74112396e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.16625938e+01 1.80926723e+01 7.86788940e+00\n",
      "  5.86702347e+00 2.64326477e+00 5.33409119e-01 0.00000000e+00]\n",
      " [1.19157333e+01 4.86143112e+00 2.61120987e+00 1.79706573e-01\n",
      "  0.00000000e+00 4.30598717e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.58626556e-01 0.00000000e+00]\n",
      " [1.43345947e+01 7.33200836e+00 6.29985809e+00 3.04705048e+00\n",
      "  5.45326233e-01 6.46268311e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 4.24467087e-01 9.77212906e-01 1.71386719e-01\n",
      "  0.00000000e+00 3.01368237e+01 1.60453033e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.83614731e-01 0.00000000e+00]\n",
      " [3.72507248e+01 1.50541306e+01 9.14283752e+00 2.65921021e+00\n",
      "  4.49298859e-01 2.58695030e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.21134949e-02\n",
      "  3.05221558e-01 5.17074471e+01 3.23171082e+01 1.13642387e+01\n",
      "  4.90679932e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.28213196e+01 1.00724792e+01 7.64702225e+00 3.56344604e+00\n",
      "  7.13199615e-01 5.17119217e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 5.59884186e+01 2.38977051e+00 7.41783142e-01\n",
      "  9.35638428e-01 1.02745819e+00 2.76435852e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  5.05287170e-01 6.89492035e+01 2.55734997e+01 1.07225990e+01\n",
      "  5.71330261e+00 4.38690186e-03 0.00000000e+00 0.00000000e+00]\n",
      " [9.80103683e+00 3.81315231e+00 1.84061813e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.73871078e+01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 2.14805603e-02 5.55152893e-02 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data \n",
    "from torch_geometric.loader import DataLoader \n",
    "import networkx as nx \n",
    "import torch \n",
    "import numpy as np \n",
    "from torch_geometric.utils import from_networkx \n",
    "\n",
    "n_vertices = 30\n",
    "G = nx.erdos_renyi_graph(n=n_vertices, p = .4) \n",
    "signal = 100 * np.random.rand(n_vertices, 1).astype(np.float32)\n",
    "nx.set_node_attributes(G, {node: {'feature': torch.tensor(signal[node])} for node in G.nodes()})\n",
    "\n",
    "geo_data = from_networkx(G)\n",
    "geo_data.x = torch.stack([node[1]['feature'] for node in G.nodes(data=True)])\n",
    "loader = DataLoader([geo_data], batch_size=1)\n",
    "\n",
    "# Run the model on the graph\n",
    "for batch in loader:\n",
    "    output, sc = model(batch)\n",
    "    # 6. Convert the output to a numpy array\n",
    "    output_numpy = output.detach().numpy()\n",
    "    print(output_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual computation of the blis output:\n",
    "from numpy import linalg as LA\n",
    "A = nx.adjacency_matrix(G).toarray()\n",
    "D_inv = np.diag(1/np.sum(A, axis = 1))\n",
    "P = 1/2 * (np.eye(n_vertices) + D_inv @ A) \n",
    "wavelets = list()\n",
    "wavelets.append(np.eye(n_vertices) - P)\n",
    "for j in range(1,5):\n",
    "    wav = LA.matrix_power(P, 2 **(j-1)) - LA.matrix_power(P, 2 ** (j))\n",
    "    wavelets.append(wav) \n",
    "wavelets.append(LA.matrix_power(P, 2 ** 4)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 1, 6])\n",
      "tensor([[[8.7951e+00, 1.9880e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8821e+01, 0.0000e+00, 0.0000e+00, 9.0223e-01, 1.9698e+00,\n",
      "          5.7615e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8835e+01, 2.1654e+01, 1.0754e+01, 8.4886e+00, 3.3004e+00,\n",
      "          4.1994e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8833e+01, 2.2086e+00, 4.9519e-01, 1.1762e-01, 1.8801e-01,\n",
      "          1.4054e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8822e+01, 2.3992e+01, 1.0460e+01, 7.2844e+00, 2.7909e+00,\n",
      "          5.1302e-01, 0.0000e+00]],\n",
      "\n",
      "        [[9.1090e+00, 6.5051e+00, 7.2047e+00, 4.2121e+00, 8.3035e-01,\n",
      "          4.8872e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.9747e+01, 1.0864e+01, 4.9507e+00, 1.4790e-01, 0.0000e+00,\n",
      "          4.8849e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.0630e-01, 0.0000e+00]],\n",
      "\n",
      "        [[1.1983e+01, 3.1010e+00, 4.6951e-02, 0.0000e+00, 0.0000e+00,\n",
      "          4.8815e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2742e+00,\n",
      "          4.4863e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9730e-01,\n",
      "          4.8863e+01, 2.3840e+01, 9.3884e+00, 5.1756e+00, 6.9995e-01,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4203e+00, 4.2879e-01,\n",
      "          4.8860e+01, 1.3083e+01, 3.7606e+00, 3.3667e-01, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[7.8134e+00, 3.0799e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8830e+01, 0.0000e+00, 0.0000e+00, 2.6432e+00, 2.3511e+00,\n",
      "          4.7056e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8839e+01, 9.5705e+00, 5.1527e+00, 4.2644e+00, 1.6546e+00,\n",
      "          1.9310e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 6.1092e-01, 9.2016e-01, 1.3957e-01,\n",
      "          4.8843e+01, 3.7399e+00, 7.9797e-01, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.1351e+01, 1.0805e+01, 8.8324e+00, 3.8383e+00, 6.4129e-01,\n",
      "          4.8866e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[1.1269e+00, 1.3667e-01, 0.0000e+00, 4.5832e-02, 3.9055e-02,\n",
      "          4.8849e+01, 0.0000e+00, 0.0000e+00, 1.3492e-02, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8853e+01, 2.3476e+01, 1.1479e+01, 8.7897e+00, 3.0609e+00,\n",
      "          2.1389e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8822e+01, 2.4825e+01, 1.0554e+01, 7.6730e+00, 3.3675e+00,\n",
      "          6.3520e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8827e+01, 7.2404e+00, 5.0742e+00, 5.3470e+00, 2.9425e+00,\n",
      "          5.5580e-01, 0.0000e+00]],\n",
      "\n",
      "        [[1.8707e+01, 1.1503e+01, 1.1195e+01, 5.6989e+00, 9.4844e-01,\n",
      "          4.8867e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.0603e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8818e+01, 0.0000e+00, 1.3623e+00, 3.3143e+00, 2.5688e+00,\n",
      "          5.7475e-01, 0.0000e+00]],\n",
      "\n",
      "        [[2.3535e+01, 1.2222e+01, 1.0293e+01, 4.4430e+00, 6.3707e-01,\n",
      "          4.8859e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8819e+01, 1.6858e+01, 7.1705e+00, 5.2347e+00, 2.3821e+00,\n",
      "          5.1526e-01, 0.0000e+00]],\n",
      "\n",
      "        [[8.3991e+00, 3.4377e+00, 1.8685e+00, 1.0085e-01, 0.0000e+00,\n",
      "          4.8829e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          1.6576e-01, 0.0000e+00]],\n",
      "\n",
      "        [[2.2195e+01, 1.1306e+01, 9.2521e+00, 3.9532e+00, 6.1730e-01,\n",
      "          4.8863e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8817e+01, 1.3208e+01, 3.6753e+00, 1.1521e+00, 4.7634e-01,\n",
      "          3.8401e-01, 0.0000e+00]],\n",
      "\n",
      "        [[2.9100e+01, 9.4398e+00, 3.2580e+00, 0.0000e+00, 5.4364e-02,\n",
      "          4.8866e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9208e-01,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8254e-02,\n",
      "          4.8857e+01, 2.7733e+01, 1.0815e+01, 5.8965e+00, 9.7465e-01,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[2.3873e+01, 1.1060e+01, 8.5027e+00, 3.6348e+00, 5.9846e-01,\n",
      "          4.8860e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[3.4376e+00, 7.8014e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          4.8831e+01, 0.0000e+00, 0.0000e+00, 6.9879e-01, 1.3421e+00,\n",
      "          3.9096e-01, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6048e-02, 2.7436e-01,\n",
      "          4.8861e+01, 1.3246e+01, 5.5371e+00, 2.9377e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[6.9912e+00, 3.4928e+00, 2.5203e+00, 7.6646e-01, 1.3637e-01,\n",
      "          4.8858e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], dtype=torch.float64)\n",
      "torch.Size([30, 1, 12])\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for wavelet in wavelets:\n",
    "    results.append(torch.tensor(wavelet @ signal))\n",
    "wavelet_transforms = torch.stack(results, dim = 2)\n",
    "print(wavelet_transforms.shape)\n",
    "m = nn.ReLU()\n",
    "s1 = m(wavelet_transforms)\n",
    "s2 = m(-wavelet_transforms)\n",
    "x = torch.cat((s1,s2), axis = -1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(output_numpy, np.array(x).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sumry2023_cqx3/blis/blis/models/torch_geo_blis_module.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmccleary-i/home/sumry2023_cqx3/blis/blis/models/torch_geo_blis_module.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output_numpy[:,:,\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39marray(x[:,:,\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "output_numpy[:,:,1] - np.array(x[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately the blis module does not agree with what I expect the output to be. I'd like to investigate the cause of this and verify that there is no bug in the implementation. There are some more serious reasons that could explain this discrepancy, but the least harmful of which would be some kind of permutation of node order. I have not ruled out any possible explanations yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUDataset definition and baseline GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 150\n",
      "Number of test graphs: 38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(7, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 2470], x=[1127, 7], edge_attr=[2470, 4], y=[64], batch=[1127], ptr=[65])\n",
      "torch.Size([1127, 7, 12])\n",
      "DataBatch(edge_index=[2, 2610], x=[1177, 7], edge_attr=[2610, 4], y=[64], batch=[1177], ptr=[65])\n",
      "torch.Size([1177, 7, 12])\n",
      "DataBatch(edge_index=[2, 914], x=[410, 7], edge_attr=[914, 4], y=[22], batch=[410], ptr=[23])\n",
      "torch.Size([410, 7, 12])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
      "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7368\n",
      "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.7632\n",
      "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
      "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
      "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7632\n",
      "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 013, Train Acc: 0.7200, Test Acc: 0.7895\n",
      "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 015, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 016, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 018, Train Acc: 0.7133, Test Acc: 0.8421\n",
      "Epoch: 019, Train Acc: 0.7400, Test Acc: 0.7895\n",
      "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 021, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 022, Train Acc: 0.7467, Test Acc: 0.7895\n",
      "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7895\n",
      "Epoch: 024, Train Acc: 0.7267, Test Acc: 0.8421\n",
      "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.8421\n",
      "Epoch: 029, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 030, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
      "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 035, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 036, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 037, Train Acc: 0.7400, Test Acc: 0.7632\n",
      "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 039, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.7368\n",
      "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.7368\n",
      "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 044, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 047, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 048, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 052, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 057, Train Acc: 0.7533, Test Acc: 0.7632\n",
      "Epoch: 058, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 059, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 060, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 062, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 065, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 067, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 070, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.8421\n",
      "Epoch: 079, Train Acc: 0.7667, Test Acc: 0.8158\n",
      "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 081, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 082, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 085, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 086, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 088, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 089, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 092, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 095, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.7895\n",
      "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.8158\n",
      "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 100, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 101, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 107, Train Acc: 0.7733, Test Acc: 0.7105\n",
      "Epoch: 108, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 109, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 111, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 112, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 115, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 116, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 117, Train Acc: 0.7733, Test Acc: 0.7895\n",
      "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7105\n",
      "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.7632\n",
      "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.7105\n",
      "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 124, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 125, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 126, Train Acc: 0.7733, Test Acc: 0.7368\n",
      "Epoch: 127, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 128, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 130, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 131, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 133, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.7632\n",
      "Epoch: 135, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 137, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 138, Train Acc: 0.8133, Test Acc: 0.7105\n",
      "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 140, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 141, Train Acc: 0.8000, Test Acc: 0.6579\n",
      "Epoch: 142, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7632\n",
      "Epoch: 144, Train Acc: 0.7867, Test Acc: 0.7368\n",
      "Epoch: 145, Train Acc: 0.8267, Test Acc: 0.7368\n",
      "Epoch: 146, Train Acc: 0.7667, Test Acc: 0.7895\n",
      "Epoch: 147, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
      "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.7105\n",
      "Epoch: 150, Train Acc: 0.7800, Test Acc: 0.7895\n",
      "Epoch: 151, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 152, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 153, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7368\n",
      "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 156, Train Acc: 0.7800, Test Acc: 0.7105\n",
      "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.7368\n",
      "Epoch: 158, Train Acc: 0.7800, Test Acc: 0.7368\n",
      "Epoch: 159, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 160, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.7632\n",
      "Epoch: 162, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 163, Train Acc: 0.7867, Test Acc: 0.7632\n",
      "Epoch: 164, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.8158\n",
      "Epoch: 166, Train Acc: 0.7733, Test Acc: 0.7632\n",
      "Epoch: 167, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 168, Train Acc: 0.7867, Test Acc: 0.7895\n",
      "Epoch: 169, Train Acc: 0.8000, Test Acc: 0.7632\n",
      "Epoch: 170, Train Acc: 0.8000, Test Acc: 0.7632\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 171):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bliss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
